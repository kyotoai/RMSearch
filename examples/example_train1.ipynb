{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa0f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example is for bigcodebench agent reflection improving\n",
    "\n",
    "!pip install .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbde7f2",
   "metadata": {},
   "source": [
    "# Step 1 : Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2468ae1d",
   "metadata": {},
   "source": [
    "## Process Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Model from local environment\n",
    "\n",
    "import asyncio\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams\n",
    "import time, warnings\n",
    "\n",
    "model_name = \"deepseek_r1_qwen14b\"\n",
    "tensor_parallel_size = 2\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model = model_name,\n",
    "    tensor_parallel_size = tensor_parallel_size,\n",
    "    gpu_memory_utilization=0.95,\n",
    ")\n",
    "engine = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "\n",
    "class AllRequests:\n",
    "    \n",
    "    def __init__(self, max_request):\n",
    "        self.max_request = max_request\n",
    "        self.requests = []\n",
    "        self.request_ids = []\n",
    "        self.request_id = 0\n",
    "        self.results = []\n",
    "        self.finished_ids = []\n",
    "        \n",
    "    def add(self, request):\n",
    "        self.requests.append(request)\n",
    "        self.request_ids.append(self.request_id)\n",
    "        self.request_id += 1\n",
    "    \n",
    "    async def process(self, model=model_name, max_tokens = 3000, temperature=0.4, save_dir = \"progress_log\", restart = False):\n",
    "\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        if restart:\n",
    "            if os.path.exists(f\"{save_dir}/finished_ids.json\") and os.path.exists(f\"{save_dir}/results.json\"):\n",
    "                with open(f\"{save_dir}/finished_ids.json\") as f:\n",
    "                    finished_ids = json.load(f)\n",
    "                with open(f\"{save_dir}/results.json\") as f:\n",
    "                    self.results = json.load(f)\n",
    "                for finished_id in finished_ids:\n",
    "                    self.finished_ids.append(finished_id)\n",
    "                    id = self.request_ids.index(finished_id)\n",
    "                    self.request_ids.pop(id)\n",
    "                    self.requests.pop(id)\n",
    "\n",
    "        await asyncio.gather(\n",
    "            *[self.process_requests(temperature = temperature, max_tokens = max_tokens, restart = restart, save_dir=save_dir) for _ in range(self.max_request)]\n",
    "        )\n",
    "            \n",
    "        return self.results\n",
    "\n",
    "\n",
    "    async def process_requests(self, max_tokens = 3000, temperature=0.4, save_dir = \"progress_log\", restart = False):\n",
    "\n",
    "        while len(self.requests) != 0:\n",
    "            request_dict = self.requests.pop(0)\n",
    "            request_id = self.request_ids.pop(0)\n",
    "\n",
    "            prompt = request_dict[\"prompt\"]\n",
    "\n",
    "            final_output = None\n",
    "            results_generator = engine.generate(prompt, SamplingParams(temperature=temperature, max_tokens=max_tokens), request_id)\n",
    "            async for request_output in results_generator:\n",
    "                # print(request_output) => for streaming\n",
    "                final_output = request_output\n",
    "\n",
    "            output = final_output.outputs[0].text\n",
    "            \n",
    "            request_dict[\"output\"] = output\n",
    "            self.results.append(request_dict)\n",
    "            self.finished_ids.append(request_id)\n",
    "\n",
    "            with open(f\"{save_dir}/results.json\", \"w\") as f:\n",
    "                json.dump(self.results, f)\n",
    "            with open(f\"{save_dir}/finished_ids.json\", \"w\") as f:\n",
    "                json.dump(self.finished_ids, f)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d38273",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171af543",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = 100\n",
    "dataset_name = 'bigcode/bigcodebench'\n",
    "save_path = \"bigcodebench3.json\"\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Load a dataset from Hugging Face\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "# Assuming you want to use the 'train' split of the dataset\n",
    "df = pd.DataFrame(dataset['v0.1.0_hf'])\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "data_list = df.to_dict(orient='records')\n",
    "#data_list = data_list[:num_sample]\n",
    "data_list = random.sample(data_list, num_sample)\n",
    "\n",
    "# Print the first few records to verify\n",
    "#print(data_list[:5])\n",
    "\n",
    "# Prepare list\n",
    "task_id = []\n",
    "complete_prompt = []\n",
    "instruct_prompt = []\n",
    "canonical_solution = []\n",
    "code_prompt = []\n",
    "test = []\n",
    "doc_struct = []\n",
    "\n",
    "for i, data_dict in enumerate(data_list):\n",
    "    task_id.append(data_dict[\"task_id\"])\n",
    "    complete_prompt.append(data_dict[\"complete_prompt\"])\n",
    "    instruct_prompt.append(data_dict[\"instruct_prompt\"])\n",
    "    canonical_solution.append(data_dict[\"canonical_solution\"])\n",
    "    code_prompt.append(data_dict[\"code_prompt\"])\n",
    "    test.append(data_dict[\"test\"])\n",
    "    doc_struct.append(data_dict[\"doc_struct\"])\n",
    "\n",
    "import json\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump({\"task_id\":task_id, \"complete_prompt\":complete_prompt, \"instruct_prompt\":instruct_prompt, \"canonical_solution\":canonical_solution, \"code_prompt\":code_prompt, \"test\":test, \"doc_struct\":doc_struct,}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070819a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_sample = 2000\n",
    "dataset_name = 'bigcode/bigcodebench'\n",
    "save_path = \"bigcodebench3.json\"\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Load a dataset from Hugging Face\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "# Assuming you want to use the 'train' split of the dataset\n",
    "df = pd.DataFrame(dataset['v0.1.3'])\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "data_list = df.to_dict(orient='records')\n",
    "#data_list = data_list[:num_sample]\n",
    "#data_list = random.sample(data_list, num_sample)\n",
    "\n",
    "# Print the first few records to verify\n",
    "#print(data_list[:5])\n",
    "\n",
    "# Prepare list\n",
    "task_id = []\n",
    "complete_prompt = []\n",
    "instruct_prompt = []\n",
    "canonical_solution = []\n",
    "code_prompt = []\n",
    "test = []\n",
    "doc_struct = []\n",
    "\n",
    "for i, data_dict in enumerate(data_list):\n",
    "    task_id.append(data_dict[\"task_id\"])\n",
    "    complete_prompt.append(data_dict[\"complete_prompt\"])\n",
    "    instruct_prompt.append(data_dict[\"instruct_prompt\"])\n",
    "    canonical_solution.append(data_dict[\"canonical_solution\"])\n",
    "    code_prompt.append(data_dict[\"code_prompt\"])\n",
    "    test.append(data_dict[\"test\"])\n",
    "    doc_struct.append(data_dict[\"doc_struct\"])\n",
    "\n",
    "import json\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump({\"task_id\":task_id, \"complete_prompt\":complete_prompt, \"instruct_prompt\":instruct_prompt, \"canonical_solution\":canonical_solution, \"code_prompt\":code_prompt, \"test\":test, \"doc_struct\":doc_struct,}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6870ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "save_path = \"bigcodebench3.json\"\n",
    "with open(save_path) as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "task_id=data_dict[\"task_id\"]\n",
    "complete_prompt=data_dict[\"complete_prompt\"]\n",
    "instruct_prompt=data_dict[\"instruct_prompt\"]\n",
    "canonical_solution=data_dict[\"canonical_solution\"]\n",
    "code_prompt=data_dict[\"code_prompt\"]\n",
    "test=data_dict[\"test\"]\n",
    "doc_struct=data_dict[\"doc_struct\"]\n",
    "\n",
    "num_problems = len(task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a3f971",
   "metadata": {},
   "source": [
    "## Solve Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb6d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids = [i for i in range(10)]\n",
    "#num_problems = 100\n",
    "ids = [i for i in range(num_problems)]\n",
    "num_sample = 1\n",
    "max_request = 15\n",
    "progress_save_dir = \"progress_log_solve4\"\n",
    "save_path = \"code-log7.json\"\n",
    "explanation = \"deepseek_r1_qwen14b/bigcodebench2 for code test with small number of samples\"\n",
    "#num_problems = len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383cfe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, os\n",
    "\n",
    "# Prepare Variables\n",
    "\n",
    "# log : { str(problem_id): { \"prompts\": {str(sample_id):promt,}, \"outputs\":{}, \"final_answers\":{}, \"corrects\":{}}, }\n",
    "log = {str(id):{\"prompts\":{}, \"outputs\":{}, \"final_answers\":{}, \"corrects\":{}, \"output_code\":{}, \"errors\":{}, \"tracebacks\":{}} for id in ids}\n",
    "log[\"num_sample\"] = num_sample\n",
    "log[\"num_problems\"] = num_problems\n",
    "log[\"info\"] = explanation\n",
    "\n",
    "\n",
    "def extract_text_inside_backticks(text, arbitrary_text):\n",
    "    # Define the pattern to match the text inside ``` that follows the arbitrary text\n",
    "    pattern = re.compile(r'```{}\\s*([\\s\\S]*?)\\s*```'.format(re.escape(arbitrary_text)))\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    match = pattern.search(text)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "'''\n",
    "def check_output(output, test_code): # this didn't work because exec behaves differently from running it in jupyter notebook\n",
    "    run_code = \"\"\"\n",
    "import inspect\n",
    "def check_code():\n",
    "    try:\n",
    "        sub_obj = TestCases()\n",
    "        for name, attribute in TestCases.__dict__.items():\n",
    "            if not name.startswith('__') and not name.startswith('_') and callable(attribute):\n",
    "                attribute(sub_obj)\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "is_code_correct__ = check_code()\"\"\"\n",
    "\n",
    "    output_code = extract_text_inside_backticks(output, \"python\")\n",
    "    if not output_code: output_code = extract_text_inside_backticks(output, \"\")\n",
    "\n",
    "    if not output_code: return False, test_code + run_code\n",
    "\n",
    "    code = output_code + \"\\n\\n\\n\" + test_code + run_code\n",
    "\n",
    "    local_vars = {}\n",
    "    global_vars = {}\n",
    "\n",
    "    try:\n",
    "        exec(test_code, global_vars, local_vars)\n",
    "        is_code_correct = local_vars['is_code_correct__']\n",
    "    except Exception as e:\n",
    "        return False, code\n",
    "\n",
    "    return is_code_correct, code\n",
    "\n",
    "\n",
    "# Define get_result (Optional): If you want to evaluate the output somehow while running process_requests, you can define get_result function and pass it to process_requests method. This will let you read log files easily with some evaluation.\n",
    "def get_result(request_dict, save_dir):\n",
    "    # request_dict: {\"prompt\":, \"output\":}\n",
    "    # save_dir: this is a directory path for progress log. put your evaluation file in here\n",
    "\n",
    "    problem_id = request_dict[\"problem_id\"]\n",
    "    output = request_dict[\"output\"]\n",
    "    test_code = request_dict[\"test_code\"]\n",
    "    \n",
    "    eval_file_path = f\"{save_dir}/score.json\"\n",
    "\n",
    "    if os.path.exists(eval_file_path):\n",
    "        with open(eval_file_path) as f:\n",
    "            log = json.load(f)\n",
    "        if str(problem_id) in log[\"num_answered\"]:\n",
    "            num_answered = log[\"num_answered\"][str(problem_id)]\n",
    "            num_correct = log[\"num_correct\"][str(problem_id)]\n",
    "        else:\n",
    "            num_answered = 0\n",
    "            num_correct = 0\n",
    "    else:\n",
    "        log = {\"num_answered\":{}, \"num_correct\":{}}\n",
    "        num_answered = 0\n",
    "        num_correct = 0\n",
    "\n",
    "    is_code_correct, code = check_output(output, test_code)\n",
    "    if is_code_correct: num_correct += 1\n",
    "\n",
    "    request_dict[\"is_code_correct\"] = is_code_correct\n",
    "\n",
    "    num_answered += 1\n",
    "    log[\"num_answered\"][str(problem_id)] = num_answered\n",
    "    log[\"num_correct\"][str(problem_id)] = num_correct\n",
    "\n",
    "    with open(eval_file_path, \"w\") as f:\n",
    "        json.dump(log, f)\n",
    "\n",
    "    return request_dict\n",
    "'''\n",
    "\n",
    "# all_requests = {str(request_id): requests}\n",
    "# requests = [{\"prompt\": , ... }, ... ]\n",
    "# Ex. all_requests = {\"0\":[{\"prompt\":, \"problem_id\":,}, ...] }\n",
    "#all_requests = {str(i):[] for i in range(max_request)}\n",
    "all_requests = AllRequests(max_request)\n",
    "\n",
    "# Prepare all_requests\n",
    "#request_id = 0\n",
    "for i in range(num_sample):  # To see the rough result quickly, it'd better process problems with different ids first. That's why loop for sample comes before one for ids.\n",
    "    for id in ids:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"{instruct_prompt[id]}\"\"\"}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        test_code = test[id]\n",
    "        \n",
    "        request_dict = {\"problem_id\":id, \"sample_id\":i, \"prompt\":prompt, \"test_code\":test_code}\n",
    "        all_requests.add(request_dict)\n",
    "        \n",
    "all_results = await all_requests.process(max_tokens = 10000, save_dir = progress_save_dir, restart = True)\n",
    "\n",
    "test_cases = []\n",
    "candidates = [[] for _ in range(num_problems)]\n",
    "test_cases_dict = {}\n",
    "# Check if the outputs are correct\n",
    "for results_dict in all_results:\n",
    "    problem_id = results_dict[\"problem_id\"]\n",
    "    sample_id = results_dict[\"sample_id\"]\n",
    "    prompt = results_dict[\"prompt\"]\n",
    "    output = results_dict[\"output\"]\n",
    "    test_code = results_dict[\"test_code\"]\n",
    "    #is_code_correct, code = check_output(output, test_code)\n",
    "\n",
    "    log[str(problem_id)][\"prompts\"][str(sample_id)] = prompt\n",
    "    log[str(problem_id)][\"outputs\"][str(sample_id)] = output\n",
    "    #log[str(problem_id)][\"corrects\"][str(sample_id)] = is_code_correct\n",
    "\n",
    "    output_code = extract_text_inside_backticks(output, \"python\")\n",
    "    if not output_code: output_code = extract_text_inside_backticks(output, \"\")\n",
    "    if not output_code: output_code = \"\"\n",
    "\n",
    "    log[str(problem_id)][\"output_code\"][str(sample_id)] = output_code\n",
    "\n",
    "    if not problem_id in test_cases_dict:\n",
    "        test_cases_dict[problem_id] = test_code\n",
    "    candidates[problem_id].append(output_code)\n",
    "\n",
    "for id in test_cases_dict:\n",
    "    test_cases.append(test_cases_dict[id])\n",
    "\n",
    "import os, time\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "#from evaluate import load\n",
    "# Load code evaluation metric\n",
    "#code_eval_metric = load(\"code_eval\")\n",
    "\n",
    "# Modified code_eval which has returns traceback of test error \n",
    "from code_eval.code_eval import CodeEval\n",
    "code_eval_metric = CodeEval()\n",
    "# Compute pass@k\n",
    "k_values = [1]\n",
    "print(\"Evaluating generated code...\")\n",
    "start = time.time()\n",
    "pass_at_k, results = code_eval_metric._compute(\n",
    "    references=test_cases,\n",
    "    predictions=candidates,\n",
    "    k=k_values,\n",
    "    num_workers=18,  # Adjust based on your system\n",
    "    timeout=100.0,   # Adjust the timeout as needed\n",
    ")\n",
    "end = time.time()\n",
    "print(\"calculation time(s): \", end-start)\n",
    "\n",
    "# Print the results\n",
    "#for k in k_values:\n",
    "#    print(f\"Pass@{k}: {pass_at_k[f'pass@{k}'] * 100:.2f}%\")\n",
    "    #log[f\"Pass@{k}\"] = pass_at_k[f'pass@{k}']\n",
    "\n",
    "total_num_correct = 0\n",
    "total_num_problem = 0\n",
    "num_correct_dict = {}\n",
    "for problem_id in range(len(results)):\n",
    "    num_correct = 0\n",
    "    for sample_id in range(len(results[problem_id])):\n",
    "        is_correct = results[problem_id][sample_id][1][\"passed\"]\n",
    "        if candidates[problem_id][sample_id]==\"\": is_correct=False  # passed become true when output_code == \"\" for some reason. This should be incorrect\n",
    "        log[str(problem_id)][\"corrects\"][str(sample_id)] = is_correct\n",
    "        if not is_correct:\n",
    "            try: # for normal case\n",
    "                log[str(problem_id)][\"errors\"][str(sample_id)] = results[problem_id][sample_id][1][\"result\"][\"error\"]\n",
    "                log[str(problem_id)][\"tracebacks\"][str(sample_id)] = results[problem_id][sample_id][1][\"result\"][\"traceback\"]\n",
    "            except: # for canse output_code == \"\"\n",
    "                log[str(problem_id)][\"errors\"][str(sample_id)] = \"failed: there is no code included in the answer\"\n",
    "                log[str(problem_id)][\"tracebacks\"][str(sample_id)] = \"failed: there is no code included in the answer\"\n",
    "        if is_correct: num_correct += 1\n",
    "    log[str(problem_id)][\"num_correct\"] = num_correct\n",
    "    num_correct_dict[str(problem_id)] = num_correct\n",
    "    total_num_correct+=num_correct\n",
    "    total_num_problem+=len(results[problem_id])\n",
    "    \n",
    "log[\"num_correct_dict\"] = num_correct_dict\n",
    "log[\"pass1\"] = total_num_correct/total_num_problem\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(log, f)\n",
    "\n",
    "print()\n",
    "print(\"-- ALL FINISHED --\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2605a21b",
   "metadata": {},
   "source": [
    "## Make Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d9a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iteration = 10\n",
    "max_request = 50  # max_request to AsyncEngine\n",
    "load_file = \"code-log5.json\"\n",
    "save_file = \"code-log5-corr1.json\"\n",
    "save_dir_base = \"progless_log9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc92ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, traceback\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "with open(load_file) as json_file:\n",
    "    log = json.load(json_file)\n",
    "\n",
    "total_num_problem = log[\"num_problems\"]\n",
    "total_num_sample = log[\"num_sample\"]*log[\"num_problems\"]\n",
    "\n",
    "if not os.path.exists(save_dir_base):\n",
    "    os.makedirs(save_dir_base)\n",
    "    \n",
    "# advice_result_log: {str(iteration):{str(problem_id):{\"num_problem\":, \"num_correct\":}}\n",
    "advice_result_log = {}\n",
    "\n",
    "def add_numbers_to_lines(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n\\n')\n",
    "\n",
    "    # Initialize a counter\n",
    "    counter = 1\n",
    "\n",
    "    # Create a list to hold the numbered lines\n",
    "    numbered_lines = []\n",
    "    numbered_texts = []\n",
    "\n",
    "    # Iterate through the lines\n",
    "    for line in lines:\n",
    "        if line.strip():  # Check if the line is not empty\n",
    "            # Add the number and the line to the list\n",
    "            numbered_lines.append((counter, line))\n",
    "            numbered_texts.append(f\"{counter}. {line}\")\n",
    "            # Increment the counter\n",
    "            counter += 1\n",
    "\n",
    "    numbered_text = '\\n\\n'.join(numbered_texts)    \n",
    "\n",
    "    return numbered_lines, numbered_text\n",
    "\n",
    "\n",
    "def get_text_before_number(numbered_lines, number):\n",
    "    # Find the index of the tuple with the given number\n",
    "    for i, (num, line) in enumerate(numbered_lines):\n",
    "        if num == number:\n",
    "            # Return the original text before the given number\n",
    "            return '\\n\\n'.join(line for _, line in numbered_lines[:i])\n",
    "\n",
    "    # If the number is not found, return an empty string\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_text_inside_backticks(text, arbitrary_text):\n",
    "    # Define the pattern to match the text inside ``` that follows the arbitrary text\n",
    "    pattern = re.compile(r'```{}\\s*([\\s\\S]*?)\\s*```'.format(re.escape(arbitrary_text)))\n",
    "\n",
    "    # Search for the pattern in the text\n",
    "    match = pattern.search(text)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Define get_result (Optional): If you want to evaluate the output somehow while running process_requests, you can define get_result function and pass it to process_requests method. This will let you read log files easily with some evaluation.\n",
    "def get_result(request_dict, save_dir):\n",
    "    # request_dict: {\"prompt\":, \"output\":}\n",
    "    # save_dir: this is a directory path for progress log. put your evaluation file in here\n",
    "\n",
    "    problem_id = int(request_dict[\"log_ids\"][0])\n",
    "    output = request_dict[\"output\"]\n",
    "    eval_file_path = f\"{save_dir}/score.json\"\n",
    "\n",
    "    if os.path.exists(eval_file_path):\n",
    "        with open(eval_file_path) as f:\n",
    "            log = json.load(f)\n",
    "        if str(problem_id) in log[\"num_answered\"]:\n",
    "            num_answered = log[\"num_answered\"][str(problem_id)]\n",
    "            num_correct = log[\"num_correct\"][str(problem_id)]\n",
    "        else:\n",
    "            num_answered = 0\n",
    "            num_correct = 0\n",
    "    else:\n",
    "        log = {\"num_answered\":{}, \"num_correct\":{}}\n",
    "        num_answered = 0\n",
    "        num_correct = 0\n",
    "\n",
    "    is_correct = False\n",
    "    pattern = r'\\\\boxed{(\\d+)}'\n",
    "    matches = re.findall(pattern, output)\n",
    "    if matches == []:\n",
    "        final_answer = None\n",
    "    else:\n",
    "        final_answer = int(matches[0])\n",
    "        if correct_answers[problem_id] == final_answer:\n",
    "            is_correct = True\n",
    "            num_correct += 1\n",
    "\n",
    "    request_dict[\"final_answer\"] = final_answer\n",
    "    request_dict[\"is_correct\"] = is_correct\n",
    "\n",
    "    num_answered += 1\n",
    "    log[\"num_answered\"][str(problem_id)] = num_answered\n",
    "    log[\"num_correct\"][str(problem_id)] = num_correct\n",
    "\n",
    "    with open(eval_file_path, \"w\") as f:\n",
    "        json.dump(log, f)\n",
    "\n",
    "    return request_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_log_dict(log, log_ids):\n",
    "    if len(log_ids) == 0:\n",
    "        return log\n",
    "    \n",
    "    problem_id = log_ids.pop(0)\n",
    "    log_dict = log[str(problem_id)]\n",
    "    \n",
    "    for log_id in log_ids:\n",
    "        log_dict = log_dict[\"children\"][str(log_id)]\n",
    "\n",
    "    return log_dict\n",
    "\n",
    "\n",
    "def get_edit_log_dict(log_ids):\n",
    "    global log\n",
    "    \n",
    "    if len(log_ids) == 0:\n",
    "        return log\n",
    "    \n",
    "    problem_id = log_ids.pop(0)\n",
    "    log_dict = log[str(problem_id)]\n",
    "    \n",
    "    for log_id in log_ids:\n",
    "        log_dict = log_dict[\"children\"][str(log_id)]\n",
    "\n",
    "    return log_dict\n",
    "\n",
    "\n",
    "\n",
    "for iter in range(num_iteration):\n",
    "\n",
    "    all_requests1 = AllRequests(max_request)\n",
    "    \n",
    "    # Make all_request for advice by searching log recursively\n",
    "    def search_log(log_dict, log_ids):  # node_id: str\n",
    "        global all_requests, request_id\n",
    "        node_id = log_ids[-1]\n",
    "\n",
    "        if type(log_dict[node_id]) != dict:\n",
    "            return None\n",
    "            \n",
    "        if \"children\" in log_dict[node_id]:\n",
    "            for next_node_id in log_dict[node_id][\"children\"]:\n",
    "                search_log(log_dict[node_id][\"children\"], log_ids + [next_node_id])\n",
    "        elif \"corrects\" in log_dict[node_id]:\n",
    "            if iter!=0:\n",
    "                all_false = True\n",
    "                for sample_id_str in log_dict[node_id][\"corrects\"]:\n",
    "                    if log_dict[node_id][\"corrects\"][sample_id_str]:\n",
    "                        all_false = False\n",
    "                        break\n",
    "                    \n",
    "                if all_false:\n",
    "                    problem_id_str = log_ids[0]\n",
    "                    problem_id = int(problem_id_str)\n",
    "                    pre_prompt = log_dict[node_id][\"prompts\"][\"0\"]\n",
    "                    pre_output = log_dict[node_id][\"outputs\"][\"0\"]\n",
    "                    error = log_dict[node_id][\"errors\"][\"0\"]\n",
    "                    traceback_ = log_dict[node_id][\"tracebacks\"][\"0\"]\n",
    "                    #student_answer = prompt.split(\"<｜Assistant｜>\")[1] + output  #log_dict[node_id][\"outputs\"][sample_id_str]\n",
    "                    #numbered_lines, numbered_answer = add_numbers_to_lines(student_answer)\n",
    "\n",
    "                    messages = [\n",
    "                        {\"role\": \"user\", \"content\": f\"\"\"### Problem:\n",
    "'''\n",
    "{instruct_prompt[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Correct Solution:\n",
    "'''\n",
    "{canonical_solution[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Student's Incorrect Answer:\n",
    "'''\n",
    "{pre_output}\n",
    "'''\n",
    "\n",
    "\n",
    "### Test Code and Its Error\n",
    "'''\n",
    "```\n",
    "{test[problem_id]}\n",
    "```\n",
    "\n",
    "{traceback_}\n",
    "'''\n",
    "\n",
    "\n",
    "You are an advanced language model tasked with analyzing a student’s answer to a coding problems and make some instructions to lead him to the correct solution. You are given the coding problem, the correct solution of it, a student’s incorrect answer, test code of the answer code and error cause of the student's incorrect answer. Please make some instructions and let him answer correctly following the instructions below.\n",
    "\n",
    "\n",
    "### Instructions:\n",
    "1. **Think Why Student’s Answer was Wrong**: Compare the correct solution and student’s incorrect answer, and analyze why the student’s answer was wrong and think about where it went in a different direction from the correct solution.\n",
    "2. **Think What was the Idea Missing in Student’s Answer**: Think what idea was included in the correct solution but was missing from the students' answer.\n",
    "3. **Imagine Many Thinking Processes Which May Lead to the Idea**: Imagine as many thinking processes as possible which may lead him to think of the missing idea.\n",
    "4. **Give Short and Abstract Instructions**: Expanding your imagination, make as many instructions as possible which may lead him to the missing idea which was not included in the student’s answer. All the instructions should be abstract and general so that it can be applied to other problems too. These are the examples of the instruction; “Explore all the possibilities of it”, “Check if there are enough conditions to solve the problem”, “Imagine what condition will lead you to solve the problem”, “Find some regularities and prove a statement which narrows down the options”, “Summarize your thought and check if it really follows the problem”.\n",
    "5. **Generate Output**: Based on the result so far, return the missing idea and Instructions in backticks like\n",
    "\n",
    "```idea\n",
    "(The missing idea in the student’s answer)\n",
    "```\n",
    "\n",
    "```instructions\n",
    "[\n",
    "    \"Instruction1 (An instruction which leads him to the missing idea)\",\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "Let’s think step by step following each step of the instructions.\"\"\"}\n",
    "                    ]\n",
    "                    \n",
    "                    prompt = tokenizer.apply_chat_template(\n",
    "                        messages,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True\n",
    "                    )\n",
    "                    \n",
    "                    new_log_ids = log_ids+[\"0\"]\n",
    "                    request_dict = {\"log_ids\":new_log_ids, \"prompt\":prompt, \"pre_prompt\":pre_prompt, \"pre_output\":pre_output}\n",
    "                    all_requests1.add(request_dict)\n",
    "                    \n",
    "            else:\n",
    "                for sample_id_str in log_dict[node_id][\"corrects\"]:\n",
    "                    if not log_dict[node_id][\"corrects\"][sample_id_str]:\n",
    "        \n",
    "                        problem_id_str = log_ids[0]\n",
    "                        problem_id = int(problem_id_str)\n",
    "                        pre_prompt = log_dict[node_id][\"prompts\"][sample_id_str]\n",
    "                        pre_output = log_dict[node_id][\"outputs\"][sample_id_str]\n",
    "                        error = log_dict[node_id][\"errors\"][sample_id_str]\n",
    "                        traceback_ = log_dict[node_id][\"tracebacks\"][sample_id_str]\n",
    "                        #student_answer = prompt.split(\"<｜Assistant｜>\")[1] + output  #log_dict[node_id][\"outputs\"][sample_id_str]\n",
    "                        #numbered_lines, numbered_answer = add_numbers_to_lines(student_answer)\n",
    "    \n",
    "                        messages = [\n",
    "                            {\"role\": \"user\", \"content\": f\"\"\"### Problem:\n",
    "'''\n",
    "{instruct_prompt[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Correct Solution:\n",
    "'''\n",
    "{canonical_solution[problem_id]}\n",
    "'''\n",
    "\n",
    "\n",
    "### Student's Incorrect Answer:\n",
    "'''\n",
    "{pre_output}\n",
    "'''\n",
    "\n",
    "\n",
    "### Test Code and Its Error\n",
    "'''\n",
    "```\n",
    "{test[problem_id]}\n",
    "```\n",
    "\n",
    "{traceback_}\n",
    "'''\n",
    "\n",
    "\n",
    "You are an advanced language model tasked with analyzing a student’s answer to a coding problems and make some instructions to lead him to the correct solution. You are given the coding problem, the correct solution of it, a student’s incorrect answer, test code of the answer code and error cause of the student's incorrect answer. Please make some instructions and let him answer correctly following the instructions below.\n",
    "\n",
    "\n",
    "### Instructions:\n",
    "1. **Think Why Student’s Answer was Wrong**: Compare the correct solution and student’s incorrect answer, and analyze why the student’s answer was wrong and think about where it went in a different direction from the correct solution.\n",
    "2. **Think What was the Idea Missing in Student’s Answer**: Think what idea was included in the correct solution but was missing from the students' answer.\n",
    "3. **Imagine Many Thinking Processes Which May Lead to the Idea**: Imagine as many thinking processes as possible which may lead him to think of the missing idea.\n",
    "4. **Give Short and Abstract Instructions**: Expanding your imagination, make as many instructions as possible which may lead him to the missing idea which was not included in the student’s answer. All the instructions should be abstract and general so that it can be applied to other problems too. These are the examples of the instruction; “Explore all the possibilities of it”, “Check if there are enough conditions to solve the problem”, “Imagine what condition will lead you to solve the problem”, “Find some regularities and prove a statement which narrows down the options”, “Summarize your thought and check if it really follows the problem”.\n",
    "5. **Generate Output**: Based on the result so far, return the missing idea and Instructions in backticks like\n",
    "\n",
    "```idea\n",
    "(The missing idea in the student’s answer)\n",
    "```\n",
    "\n",
    "```instructions\n",
    "[\n",
    "    \"Instruction1 (An instruction which leads him to the missing idea)\",\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "Let’s think step by step following each step of the instructions.\"\"\"}\n",
    "                        ]\n",
    "                        \n",
    "                        prompt = tokenizer.apply_chat_template(\n",
    "                            messages,\n",
    "                            tokenize=False,\n",
    "                            add_generation_prompt=True\n",
    "                        )\n",
    "                        \n",
    "                        new_log_ids = log_ids+[sample_id_str]\n",
    "                        request_dict = {\"log_ids\":new_log_ids, \"prompt\":prompt, \"pre_prompt\":pre_prompt, \"pre_output\":pre_output}\n",
    "                        all_requests1.add(request_dict)\n",
    "    \n",
    "    for problem_id_str in log:\n",
    "        search_log(log, [problem_id_str])\n",
    "    \n",
    "    \n",
    "    # Process all_requests\n",
    "    # If you had some error in last process and want to continue to get the output, set restart = True\n",
    "    all_results1 = await all_requests1.process(max_tokens = 15000, restart = True, save_dir=f\"{save_dir_base}/inst-{iter}\")\n",
    "    all_requests2 = AllRequests(max_request)\n",
    "\n",
    "    error_num = 0\n",
    "    num_results = len(all_results1)\n",
    "    for result_dict in all_results1:\n",
    "        log_ids = result_dict[\"log_ids\"]\n",
    "        prompt = result_dict[\"prompt\"]\n",
    "        output = result_dict[\"output\"]\n",
    "        pre_prompt = result_dict[\"pre_prompt\"]\n",
    "        pre_output = result_dict[\"pre_output\"]\n",
    "        instruction_log = prompt + output\n",
    "\n",
    "        missing_idea = extract_text_inside_backticks(output, \"idea\")\n",
    "        instruction_list_text = extract_text_inside_backticks(output, \"instructions\")\n",
    "        \n",
    "        if missing_idea and instruction_list_text:\n",
    "            try:\n",
    "                instruction_list_text = instruction_list_text.replace(\"\\n\",\"\")\n",
    "                instruction_list_text = instruction_list_text.replace(\"\\\\\",\"\")\n",
    "                pattern = r\"'((?:[^']|'(?!\\s*[,\\]]))*)'\"\n",
    "                replacement = r'\"\\1\"'\n",
    "                #instruction_list_text = re.sub(pattern, replacement, instruction_list_text)  # convert ['I'm a cat', 'This is the student's car',] into [\"I'm a cat\", \"This is the student's car\",]\n",
    "                instruction_list = json.loads(instruction_list_text)\n",
    "            except Exception as e:\n",
    "                traceback.print_exc()\n",
    "                error_num += 1\n",
    "                print()\n",
    "                print(\"An error occurred:\", e)\n",
    "                print(\"instruction_list_text: \", instruction_list_text)\n",
    "                print(\"error_num: \", error_num)\n",
    "                continue\n",
    "    \n",
    "            problem_id = int(log_ids[0])\n",
    "            problem = instruct_prompt[problem_id]\n",
    "\n",
    "            prompts_dict = {}\n",
    "            insert_ids=[]\n",
    "            instruction_ids=[]\n",
    "            prompt_id = 0\n",
    "            for instruction_id, instruction in enumerate(instruction_list):\n",
    "                modified_insts = instruction_list[:instruction_id] + instruction_list[(instruction_id+1):]\n",
    "                advices_text = \"\"\n",
    "                for i, inst in enumerate(modified_insts):\n",
    "                    advices_text += f\"\\nAdvice{i+1}: {inst}\"\n",
    "                    \n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": pre_prompt[29:][:-13]},\n",
    "                    {\"role\": \"assistant\", \"content\": pre_output},\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"Your answer might contain some errors. Please revise your answer following the advice below;\n",
    "{advices_text}\"\"\"}\n",
    "                ]\n",
    "                    \n",
    "                new_prompt = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "\n",
    "                prompts_dict[str(prompt_id)] = new_prompt\n",
    "\n",
    "                next_request = {\"log_ids\":log_ids+[str(instruction_id)], \"prompt\":new_prompt, \"instruction_log\":[instruction_log]}\n",
    "                all_requests2.add(next_request)\n",
    "                \n",
    "            edit_log_dict = get_edit_log_dict(log_ids[:-1])\n",
    "            if \"children\" in edit_log_dict:\n",
    "                edit_log_dict[\"children\"][str(log_ids[-1])] = {\"instruction_list\":instruction_list, \"prompts\":prompts_dict, \"instruction_log\":[instruction_log]}\n",
    "            else:\n",
    "                edit_log_dict[\"children\"] = {str(log_ids[-1]):{\"instruction_list\":instruction_list, \"prompts\":prompts_dict, \"instruction_log\":[instruction_log]}}\n",
    "\n",
    "\n",
    "    with open(save_file, \"w\") as f:\n",
    "        json.dump(log, f)\n",
    "\n",
    "    print(\"log saved\")\n",
    "    \n",
    "    # Process all_requests\n",
    "    # If you had trouble in last process and want to continue to get the output, set restart = True\n",
    "    all_results2 = await all_requests2.process(max_tokens = 15000, restart = True, save_dir=f\"{save_dir_base}/solve-{iter}\")\n",
    "\n",
    "    advice_result_log[str(iter)] = {\"num_problem\":{}, \"num_correct\":{},}\n",
    "    test_cases = []\n",
    "    candidates = []\n",
    "    problem_ids = []\n",
    "    sample_ids = []\n",
    "    log_ids_list = []\n",
    "    # Check if the outputs are correct\n",
    "    for results_dict in all_results2:\n",
    "        log_ids = results_dict[\"log_ids\"]\n",
    "        prompt = results_dict[\"prompt\"]\n",
    "        output = results_dict[\"output\"]\n",
    "        problem_id = int(log_ids[0])\n",
    "        sample_id = int(log_ids[-1])\n",
    "        test_code = test[problem_id]\n",
    "\n",
    "        output_code = extract_text_inside_backticks(output, \"python\")\n",
    "        if not output_code: output_code = extract_text_inside_backticks(output, \"\")\n",
    "        if not output_code: output_code = \"\"\n",
    "\n",
    "        edit_log_dict = get_edit_log_dict(log_ids[:-1])\n",
    "\n",
    "        if \"prompts\" in edit_log_dict:\n",
    "            edit_log_dict[\"prompts\"][str(sample_id)] = prompt\n",
    "        else:\n",
    "            edit_log_dict[\"prompts\"] = {str(sample_id):prompt}\n",
    "\n",
    "        if \"outputs\" in edit_log_dict:\n",
    "            edit_log_dict[\"outputs\"][str(sample_id)] = output\n",
    "        else:\n",
    "            edit_log_dict[\"outputs\"] = {str(sample_id):output}\n",
    "\n",
    "        if \"output_codes\" in edit_log_dict:\n",
    "            edit_log_dict[\"output_codes\"][str(sample_id)] = output_code\n",
    "        else:\n",
    "            edit_log_dict[\"output_codes\"] = {str(sample_id):output_code}\n",
    "\n",
    "        test_cases.append(test_code)\n",
    "        candidates.append([output_code])\n",
    "        problem_ids.append(problem_id)\n",
    "        sample_ids.append(sample_id)\n",
    "        log_ids_list.append(log_ids)\n",
    "    \n",
    "    import os, time\n",
    "    os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "    from code_eval.code_eval import CodeEval\n",
    "    code_eval_metric = CodeEval()\n",
    "    # Compute pass@k\n",
    "    k_values = [1]\n",
    "    print(\"Evaluating generated code...\")\n",
    "    start = time.time()\n",
    "    pass_at_k, results = code_eval_metric._compute(\n",
    "        references=test_cases,\n",
    "        predictions=candidates,\n",
    "        k=k_values,\n",
    "        num_workers=10,  # Adjust based on your system\n",
    "        timeout=150.0,   # Adjust the timeout as needed\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(\"calculation time(s): \", end-start)\n",
    "    \n",
    "    for i in range(len(results)):\n",
    "        problem_id = problem_ids[i]\n",
    "        sample_id = sample_ids[i]\n",
    "        unexpected_error = False\n",
    "        if results[problem_id] == []:\n",
    "            is_correct = False  # [] appeared sometimes for unknown reason. I define it as incorrect for now, but it should be fixed.\n",
    "            unexpected_error = True\n",
    "        else: is_correct = results[problem_id][0][1][\"passed\"]\n",
    "        \n",
    "        log_ids = log_ids_list[i]\n",
    "        edit_log_dict = get_edit_log_dict(log_ids[:-1])\n",
    "\n",
    "        if \"corrects\" in edit_log_dict:\n",
    "            edit_log_dict[\"corrects\"][str(sample_id)] = is_correct\n",
    "        else:\n",
    "            edit_log_dict[\"corrects\"] = {str(sample_id):is_correct}\n",
    "\n",
    "        if not is_correct:\n",
    "            if not unexpected_error:\n",
    "                error = results[problem_id][0][1][\"result\"][\"error\"]\n",
    "                traceback_ = results[problem_id][0][1][\"result\"][\"traceback\"]\n",
    "            else:\n",
    "                error = \"\"\n",
    "                traceback_ = \"\"\n",
    "    \n",
    "            if \"errors\" in edit_log_dict:\n",
    "                edit_log_dict[\"errors\"][str(sample_id)] = error\n",
    "            else:\n",
    "                edit_log_dict[\"errors\"] = {str(sample_id):error}\n",
    "    \n",
    "            if \"tracebacks\" in edit_log_dict:\n",
    "                edit_log_dict[\"tracebacks\"][str(sample_id)] = traceback_\n",
    "            else:\n",
    "                edit_log_dict[\"tracebacks\"] = {str(sample_id):traceback_}\n",
    "    \n",
    "        if str(log_ids[0]) in advice_result_log[str(iter)][\"num_correct\"]:\n",
    "            if is_correct:\n",
    "                advice_result_log[str(iter)][\"num_correct\"][str(log_ids[0])] += 1\n",
    "        else:\n",
    "            if is_correct:\n",
    "                advice_result_log[str(iter)][\"num_correct\"][str(log_ids[0])] = 1\n",
    "            else:\n",
    "                advice_result_log[str(iter)][\"num_correct\"][str(log_ids[0])] = 0\n",
    "\n",
    "        if str(log_ids[0]) in advice_result_log[str(iter)][\"num_problem\"]:\n",
    "            advice_result_log[str(iter)][\"num_problem\"][str(log_ids[0])] += 1\n",
    "        else:\n",
    "            advice_result_log[str(iter)][\"num_problem\"][str(log_ids[0])] = 1\n",
    "\n",
    "\n",
    "    num_problem = 0\n",
    "    num_sample = 0\n",
    "    pass1_count = 0\n",
    "    passAll_count = 0\n",
    "    for problem_id_str in advice_result_log[str(iter)][\"num_problem\"]:\n",
    "        num_problem += 1\n",
    "        num_sample += advice_result_log[str(iter)][\"num_problem\"][problem_id_str]\n",
    "    for problem_id_str in advice_result_log[str(iter)][\"num_correct\"]:\n",
    "        pass1_count += advice_result_log[str(iter)][\"num_correct\"][problem_id_str]\n",
    "        if advice_result_log[str(iter)][\"num_correct\"][problem_id_str] > 0:\n",
    "            passAll_count += 1\n",
    "\n",
    "    num_already_correct_problem = total_num_problem - len(all_results1)\n",
    "\n",
    "    print(\"total_num_problem: \", total_num_problem)\n",
    "    print(\"total_num_sample: \", total_num_sample)\n",
    "    print(\"num_already_correct_problem: \", num_already_correct_problem)\n",
    "    print(f\"{passAll_count}/{num_problem} problems have got at least 1 correct sample in this iteration\")\n",
    "    print(f\"{pass1_count}/{num_sample} samples were correct in total\")\n",
    "    \n",
    "    pass1 = pass1_count/num_sample\n",
    "    passAll = passAll_count/num_problem\n",
    "\n",
    "    advice_result_log[str(iter)][\"pass@1\"] = pass1\n",
    "    advice_result_log[str(iter)][\"passAll\"] = passAll\n",
    "    print(\"pass1 in this iteration: \", pass1)\n",
    "    print(\"passAll in this iteration: \", passAll)\n",
    "    \n",
    "    \n",
    "    log[\"advice_result_log\"] = advice_result_log\n",
    "    with open(save_file, \"w\") as json_file:\n",
    "        json.dump(log, json_file)\n",
    "\n",
    "print()\n",
    "print(\"-- ALL FINISHED --\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32bfef5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Step 2 : Make dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39619091",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = \"/workspace/exp12\"\n",
    "model_save_dir = f\"{exp_dir}/model1\"\n",
    "model_name = \"/workspace/llama3b-rm\"\n",
    "data_save_path = \"/workspace/bigcodebench3.json\"\n",
    "advice_save_path = \"/workspace/code-log5-corr1.json\"\n",
    "data_dict_save_path = f\"{exp_dir}/data_dict.json\"\n",
    "dataset_list_save_path = f\"{exp_dir}/dataset_list.json\"\n",
    "#num_advice_per_batch = 13  # total number of advice including both chosen and rejected per 1 batch\n",
    "avoid_topk = 10 # avoid topk similar advices to each advice list being included in rejected advice\n",
    "\n",
    "import json, os\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab164ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference\n",
    "# https://medium.com/towards-generative-ai/reward-model-training-2209d1befb5f\n",
    "\n",
    "if not os.path.exists(data_dict_save_path):\n",
    "    \n",
    "    with open(data_save_path) as f:\n",
    "        data_dict = json.load(f)\n",
    "    \n",
    "    task_id=data_dict[\"task_id\"]\n",
    "    complete_prompt=data_dict[\"complete_prompt\"]\n",
    "    instruct_prompt=data_dict[\"instruct_prompt\"]\n",
    "    canonical_solution=data_dict[\"canonical_solution\"]\n",
    "    code_prompt=data_dict[\"code_prompt\"]\n",
    "    test=data_dict[\"test\"]\n",
    "    doc_struct=data_dict[\"doc_struct\"]\n",
    "    num_problems = len(task_id)\n",
    "    \n",
    "    with open(advice_save_path) as f:\n",
    "        log = json.load(f)\n",
    "    \n",
    "    data_dict = {\"problem_id\":[], \"problem\":[], \"advice\":[], \"advice_id\":[], \"output\":[], \"correct\":[], \"log_ids\":[], \"node_state\":[]}\n",
    "    \n",
    "    def search_log(log_dict, log_ids):  # node_id: str\n",
    "        global data_dict\n",
    "        node_id = log_ids[-1]\n",
    "    \n",
    "        if type(log_dict[node_id]) != dict:\n",
    "            return None\n",
    "            \n",
    "        if \"children\" in log_dict[node_id]:\n",
    "            for next_node_id in log_dict[node_id][\"children\"]:\n",
    "                problem_id_str = log_ids[0]\n",
    "                problem_id = int(problem_id_str)\n",
    "                problem = instruct_prompt[problem_id]\n",
    "                advice_list = log_dict[node_id][\"children\"][next_node_id][\"instruction_list\"]\n",
    "    \n",
    "                if \"corrects\" in log_dict[node_id][\"children\"][next_node_id]:  # when quiting the inference after finihing creating advice\n",
    "                    all_correct = True\n",
    "                    all_incorrect = True\n",
    "                    child_num_correct = 0\n",
    "                    child_num_problem = 0\n",
    "                    for advice_id, advice in enumerate(advice_list):\n",
    "                        correct = log_dict[node_id][\"children\"][next_node_id][\"corrects\"][str(advice_id)]\n",
    "                        child_num_problem += 1\n",
    "                        if correct:\n",
    "                            all_incorrect = False\n",
    "                            child_num_correct += 1\n",
    "                        else:\n",
    "                            all_correct = False\n",
    "    \n",
    "                    if child_num_problem == 0: continue\n",
    "    \n",
    "                    for advice_id, advice in enumerate(advice_list):\n",
    "                        correct = log_dict[node_id][\"children\"][next_node_id][\"corrects\"][str(advice_id)]\n",
    "                        #output = log_dict[node_id][\"children\"][next_node_id][\"outputs\"][str(advice_id)]\n",
    "                        output = log_dict[node_id][\"outputs\"][\"0\"]\n",
    "                        data_dict[\"problem_id\"].append(problem_id)\n",
    "                        data_dict[\"problem\"].append(problem)\n",
    "                        data_dict[\"output\"].append(output)\n",
    "                        data_dict[\"advice\"].append(advice)\n",
    "                        data_dict[\"advice_id\"].append(advice_id)\n",
    "                        data_dict[\"correct\"].append(correct)\n",
    "                        data_dict[\"log_ids\"].append(log_ids+[next_node_id])\n",
    "                        data_dict[\"node_state\"].append({\"num_problem\":len(advice_list), \"child_num_correct\":child_num_correct, \"all_correct\":all_correct, \"all_incorrect\":all_incorrect})\n",
    "    \n",
    "                    if all_incorrect:\n",
    "                        search_log(log_dict[node_id][\"children\"], log_ids + [next_node_id])\n",
    "    \n",
    "    problem_ids_with_advice = []\n",
    "    for problem_id_str in log:\n",
    "        if type(log[problem_id_str])==dict:\n",
    "            if \"children\" in log[problem_id_str]:\n",
    "                problem_ids_with_advice.append(int(problem_id_str))\n",
    "                search_log(log, [problem_id_str])\n",
    "    \n",
    "    with open(f\"{exp_dir}/data_dict.json\", \"w\") as f: json.dump(data_dict, f)\n",
    "    with open(f\"{exp_dir}/problem_ids_with_advice.json\", \"w\") as f: json.dump(problem_ids_with_advice, f)\n",
    "\n",
    "else:\n",
    "    with open(f\"{exp_dir}/data_dict.json\") as f: data_dict = json.load(f)\n",
    "    with open(f\"{exp_dir}/problem_ids_with_advice.json\") as f: problem_ids_with_advice = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd5fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1399/4218478872.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  advice_similarities = torch.load(f\"{exp_dir}/advice_similarities.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if not os.path.exists(f\"{exp_dir}/advice_similarities.pt\"):\n",
    "    # Get similarities between advice so that avoid rejected advice being important for the problems\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sentence_transformers.util import cos_sim\n",
    "    from sentence_transformers.quantization import quantize_embeddings\n",
    "    \n",
    "    # 1. Specify preffered dimensions\n",
    "    dimensions = 512\n",
    "    \n",
    "    # 2. load model\n",
    "    model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim=dimensions).to(\"cuda\")\n",
    "    advices = data_dict[\"advice\"]\n",
    "    # The prompt used for query retrieval tasks:\n",
    "    # query_prompt = 'Represent this sentence for searching relevant passages: '\n",
    "    \n",
    "    # 2. Encode\n",
    "    query_embedding = model.encode(advices, prompt_name=\"query\")\n",
    "    # Equivalent Alternatives:\n",
    "    # query_embedding = model.encode(query_prompt + query)\n",
    "    # query_embedding = model.encode(query, prompt=query_prompt)\n",
    "    \n",
    "    docs_embeddings = model.encode(advices)\n",
    "    \n",
    "    # Optional: Quantize the embeddings\n",
    "    binary_query_embedding = quantize_embeddings(query_embedding, precision=\"ubinary\")\n",
    "    binary_query_embedding = torch.tensor(binary_query_embedding).to(\"cuda\")\n",
    "    binary_docs_embeddings = quantize_embeddings(docs_embeddings, precision=\"ubinary\")\n",
    "    binary_docs_embeddings = torch.tensor(binary_docs_embeddings).to(\"cuda\")\n",
    "    \n",
    "    advice_similarities = cos_sim(query_embedding, docs_embeddings)\n",
    "    torch.save(advice_similarities, f\"{exp_dir}/advice_similarities.pt\")\n",
    "    \n",
    "else:\n",
    "    advice_similarities = torch.load(f\"{exp_dir}/advice_similarities.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "if not os.path.exists(dataset_list_save_path):\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    df['id'] = range(len(df))\n",
    "    dataset_list = []\n",
    "    \n",
    "    for problem_id in problem_ids_with_advice:\n",
    "        #problem = instruct_prompt[problem_id]\n",
    "        chosen_df = df[df['problem_id'] == problem_id]\n",
    "        problem = chosen_df['problem'].values[0]\n",
    "        output = chosen_df['output'].values[0]\n",
    "        chosen_ids = chosen_df[\"id\"].tolist()\n",
    "        all_avoid_ids = []\n",
    "        for chosen_id in chosen_ids:\n",
    "            _, avoid_ids = torch.topk(advice_similarities[chosen_id], k=avoid_topk)\n",
    "            avoid_ids = avoid_ids.tolist()\n",
    "            all_avoid_ids += avoid_ids\n",
    "        \n",
    "        rejected_df = df.drop(all_avoid_ids)[df['problem_id'] != problem_id]\n",
    "        num_chosen = len(chosen_df)\n",
    "        num_rejected = num_chosen\n",
    "        if not chosen_df.empty and not rejected_df.empty:\n",
    "            '''\n",
    "            if len_chosen_df<4:\n",
    "                num_chosen = len_chosen_df\n",
    "                num_rejected = num_advice_per_batch - len_chosen_df\n",
    "            else: # For the case there are too many chosen advices\n",
    "                num_chosen = 3\n",
    "                num_rejected = num_advice_per_batch - num_chosen\n",
    "            '''\n",
    "            chosen_rows = chosen_df.sample(n=num_chosen)\n",
    "            rejected_rows = rejected_df.sample(n=num_rejected)\n",
    "            chosen_advice_ids = chosen_rows[\"id\"].tolist()\n",
    "            rejected_advice_ids = rejected_rows[\"id\"].tolist()\n",
    "            #chosen_reject_similarities = advice_similarities[chosen_ids][:, rejected_ids]\n",
    "            for i in range(num_chosen):\n",
    "                chosen_advice = chosen_rows['advice'].values[i]\n",
    "                rejected_advice = rejected_rows['advice'].values[i]\n",
    "                chosen_advice_id = chosen_advice_ids[i]\n",
    "                rejected_advice_id = rejected_advice_ids[i]\n",
    "                \n",
    "                dataset_list.append({\n",
    "                    \"query\":[{'role': 'user', 'content':f\"Give me an advice to the problem and answer below;\\n\\nProblem:{problem}\\n\\nAnswer:{output}\"}],\n",
    "                    \"chosen_key\":[{'role': 'assistant', 'content': f\"{chosen_advice}\"}],\n",
    "                    \"rejected_key\":[{'role': 'assistant', 'content': f\"{rejected_advice}\"}],\n",
    "                    \"problem_id\":problem_id,\n",
    "                    \"problem\":problem,\n",
    "                    \"output\":output,\n",
    "                    \"chosen_advice\":chosen_advice,\n",
    "                    \"rejected_advice\":rejected_advice,\n",
    "                    \"chosen_advice_id\":chosen_advice_id,\n",
    "                    \"rejected_advice_id\":rejected_advice_id,\n",
    "                })\n",
    "        else:\n",
    "            #print(f\"problem {problem_id} doesn't have any advice\")\n",
    "            continue\n",
    "    \n",
    "    #if num_gpu*batch_size_per_device != 1:\n",
    "    #    num_trash = len(dataset_list)%(num_gpu*batch_size_per_device)\n",
    "    #    dataset_list = dataset_list[:-num_trash]\n",
    "    \n",
    "    with open(dataset_list_save_path, \"w\") as f:\n",
    "        json.dump(dataset_list, f)\n",
    "        \n",
    "else:\n",
    "    with open(dataset_list_save_path) as f:\n",
    "        dataset_list = json.load(f)\n",
    "\n",
    "#dataset1 = Dataset.from_list(dataset_list)\n",
    "#dataset1.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60cb68",
   "metadata": {},
   "source": [
    "# Step 3 : Train Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845d0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0578ac074b514a20b053bbe67cc43ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rmsearch import RMTrainer\n",
    "\n",
    "model_name = \"/workspace/llama3b-rm\"\n",
    "num_gpus = 1\n",
    "\n",
    "rmtrainer = RMTrainer(model_name = model_name, num_gpus = num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85889110",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_save_path = f\"{exp_dir}/dataset\"\n",
    "train_ids_save_path = f\"{exp_dir}/train_ids.json\"\n",
    "test_ids_save_path = f\"{exp_dir}/test_ids.json\"\n",
    "test_size = 48\n",
    "\n",
    "formatted_dataset = rmtrainer.prepare_dataset(dataset_list, dataset_save_path, test_size, train_ids_save_path, test_ids_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9b497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> chosen_text                                   </span>┃<span style=\"font-weight: bold\"> rejected_text                                </span>┃<span style=\"font-weight: bold\"> logits           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ &lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|… │ &lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;… │ [0.4907, 0.5093] │\n",
       "│                                               │                                              │                  │\n",
       "│ Cutting Knowledge Date: December 2023         │ Cutting Knowledge Date: December 2023        │                  │\n",
       "│ Today Date: 07 May 2025                       │ Today Date: 07 May 2025                      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ &lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_heade… │ &lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_head… │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Give me an advice to the problem and answer   │ Give me an advice to the problem and answer  │                  │\n",
       "│ below;                                        │ below;                                       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Problem:Train a logistic regression model on  │ Problem:Train a logistic regression model on │                  │\n",
       "│ one feature and evaluate its performance      │ one feature and evaluate its performance     │                  │\n",
       "│ using a confusion matrix plot. The function   │ using a confusion matrix plot. The function  │                  │\n",
       "│ takes a feature and a target series, splits   │ takes a feature and a target series, splits  │                  │\n",
       "│ them into training and testing sets, trains   │ them into training and testing sets, trains  │                  │\n",
       "│ the logistic regression model, predicts the   │ the logistic regression model, predicts the  │                  │\n",
       "│ target for the test set, and plots the        │ target for the test set, and plots the       │                  │\n",
       "│ confusion matrix.                             │ confusion matrix.                            │                  │\n",
       "│ The function should output with:              │ The function should output with:             │                  │\n",
       "│     (np.ndarray, plt.Axes): A tuple           │     (np.ndarray, plt.Axes): A tuple          │                  │\n",
       "│ containing the confusion matrix and the       │ containing the confusion matrix and the      │                  │\n",
       "│ matplotlib Axes object of the confusion       │ matplotlib Axes object of the confusion      │                  │\n",
       "│ matrix plot.                                  │ matrix plot.                                 │                  │\n",
       "│ You should write self-contained code starting │ You should write self-contained code         │                  │\n",
       "│ with:                                         │ starting with:                               │                  │\n",
       "│ ```                                           │ ```                                          │                  │\n",
       "│ import pandas as pd                           │ import pandas as pd                          │                  │\n",
       "│ from sklearn.model_selection import           │ from sklearn.model_selection import          │                  │\n",
       "│ train_test_split                              │ train_test_split                             │                  │\n",
       "│ from sklearn.linear_model import              │ from sklearn.linear_model import             │                  │\n",
       "│ LogisticRegression                            │ LogisticRegression                           │                  │\n",
       "│ from sklearn.metrics import confusion_matrix  │ from sklearn.metrics import confusion_matrix │                  │\n",
       "│ import numpy as np                            │ import numpy as np                           │                  │\n",
       "│ import matplotlib.pyplot as plt               │ import matplotlib.pyplot as plt              │                  │\n",
       "│ def task_func(feature: pd.Series, target:     │ def task_func(feature: pd.Series, target:    │                  │\n",
       "│ pd.Series) -&gt; (np.ndarray, plt.Axes):         │ pd.Series) -&gt; (np.ndarray, plt.Axes):        │                  │\n",
       "│ ```                                           │ ```                                          │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Answer:&lt;think&gt;                                │ Answer:&lt;think&gt;                               │                  │\n",
       "│ Okay, I need to write a Python function       │ Okay, I need to write a Python function      │                  │\n",
       "│ called task_func that takes a feature and a   │ called task_func that takes a feature and a  │                  │\n",
       "│ target as inputs and returns a tuple          │ target as inputs and returns a tuple         │                  │\n",
       "│ containing a confusion matrix and a           │ containing a confusion matrix and a          │                  │\n",
       "│ matplotlib Axes object. The function should   │ matplotlib Axes object. The function should  │                  │\n",
       "│ train a logistic regression model on one      │ train a logistic regression model on one     │                  │\n",
       "│ feature and evaluate its performance using a  │ feature and evaluate its performance using a │                  │\n",
       "│ confusion matrix plot.                        │ confusion matrix plot.                       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ First, I should think about the steps         │ First, I should think about the steps        │                  │\n",
       "│ involved. Let's break it down.                │ involved. Let's break it down.               │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ The function starts by importing necessary    │ The function starts by importing necessary   │                  │\n",
       "│ libraries, which are already provided. So I   │ libraries, which are already provided. So I  │                  │\n",
       "│ don't need to worry about that.               │ don't need to worry about that.              │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ The first step is to split the feature and    │ The first step is to split the feature and   │                  │\n",
       "│ target into training and testing sets. I      │ target into training and testing sets. I     │                  │\n",
       "│ remember that the train_test_split function   │ remember that the train_test_split function  │                  │\n",
       "│ from sklearn can do this. So I'll use that.   │ from sklearn can do this. So I'll use that.  │                  │\n",
       "│ The feature is a pandas Series, so I need to  │ The feature is a pandas Series, so I need to │                  │\n",
       "│ convert it into a DataFrame or maybe just an  │ convert it into a DataFrame or maybe just an │                  │\n",
       "│ array? Wait, no. Wait, the feature is a       │ array? Wait, no. Wait, the feature is a      │                  │\n",
       "│ single feature, right? So when I split, I can │ single feature, right? So when I split, I    │                  │\n",
       "│ pass the feature as X and target as y.        │ can pass the feature as X and target as y.   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Wait, but train_test_split expects X and y to │ Wait, but train_test_split expects X and y   │                  │\n",
       "│ be arrays. So for the feature, perhaps I      │ to be arrays. So for the feature, perhaps I  │                  │\n",
       "│ should convert the pd.Series into a numpy     │ should convert the pd.Series into a numpy    │                  │\n",
       "│ array. Because when I pass a Series to        │ array. Because when I pass a Series to       │                  │\n",
       "│ train_test_split, it might work, but          │ train_test_split, it might work, but         │                  │\n",
       "│ sometimes it's better to convert it to a      │ sometimes it's better to convert it to a     │                  │\n",
       "│ numpy array. Alternatively, I can reshape it  │ numpy array. Alternatively, I can reshape it │                  │\n",
       "│ into a 2D array since scikit-learn expects 2D │ into a 2D array since scikit-learn expects   │                  │\n",
       "│ for features.                                 │ 2D for features.                             │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Wait, the feature is a single feature, so     │ Wait, the feature is a single feature, so    │                  │\n",
       "│ it's a 1D array. So when I pass it to         │ it's a 1D array. So when I pass it to        │                  │\n",
       "│ train_test_split, I need to make sure it's in │ train_test_split, I need to make sure it's   │                  │\n",
       "│ the correct shape. So perhaps I should        │ in the correct shape. So perhaps I should    │                  │\n",
       "│ reshape it into a 2D array with shape         │ reshape it into a 2D array with shape        │                  │\n",
       "│ (n_samples, 1). Because LogisticRegression    │ (n_samples, 1). Because LogisticRegression   │                  │\n",
       "│ can handle that.                              │ can handle that.                             │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ So, for X, I'll take                          │ So, for X, I'll take                         │                  │\n",
       "│ feature.values.reshape(-1, 1), and y is       │ feature.values.reshape(-1, 1), and y is      │                  │\n",
       "│ target.values.                                │ target.values.                               │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Then, split them into training and testing    │ Then, split them into training and testing   │                  │\n",
       "│ sets. I can set a random state for            │ sets. I can set a random state for           │                  │\n",
       "│ reproducibility, but it's not specified, so   │ reproducibility, but it's not specified, so  │                  │\n",
       "│ maybe I can leave it out or set it to a       │ maybe I can leave it out or set it to a      │                  │\n",
       "│ specific value like 42 for consistency.       │ specific value like 42 for consistency.      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Next, I need to train the logistic regression │ Next, I need to train the logistic           │                  │\n",
       "│ model. So I'll create an instance of          │ regression model. So I'll create an instance │                  │\n",
       "│ LogisticRegression. The default parameters    │ of LogisticRegression. The default           │                  │\n",
       "│ should be fine, but sometimes it's good to    │ parameters should be fine, but sometimes     │                  │\n",
       "│ set the solver, like 'lbfgs'. But I think the │ it's good to set the solver, like 'lbfgs'.   │                  │\n",
       "│ default is okay.                              │ But I think the default is okay.             │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Then, fit the model on the training data. So  │ Then, fit the model on the training data. So │                  │\n",
       "│ model.fit(X_train, y_train).                  │ model.fit(X_train, y_train).                 │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ After training, I need to predict the target  │ After training, I need to predict the target │                  │\n",
       "│ for the test set. So y_pred =                 │ for the test set. So y_pred =                │                  │\n",
       "│ model.predict(X_test).                        │ model.predict(X_test).                       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Then, compute the confusion matrix using      │ Then, compute the confusion matrix using     │                  │\n",
       "│ confusion_matrix from sklearn.metrics. So cm  │ confusion_matrix from sklearn.metrics. So cm │                  │\n",
       "│ = confusion_matrix(y_test, y_pred).           │ = confusion_matrix(y_test, y_pred).          │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Now, I need to plot the confusion matrix. How │ Now, I need to plot the confusion matrix.    │                  │\n",
       "│ do I do that? I remember that a heatmap is a  │ How do I do that? I remember that a heatmap  │                  │\n",
       "│ good way to visualize it. So I can use        │ is a good way to visualize it. So I can use  │                  │\n",
       "│ matplotlib's imshow function, or perhaps use  │ matplotlib's imshow function, or perhaps use │                  │\n",
       "│ seaborn's heatmap for better aesthetics.      │ seaborn's heatmap for better aesthetics.     │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Wait, but the function needs to return the    │ Wait, but the function needs to return the   │                  │\n",
       "│ confusion matrix as a numpy array and the     │ confusion matrix as a numpy array and the    │                  │\n",
       "│ Axes object. So I'll need to create a plot,   │ Axes object. So I'll need to create a plot,  │                  │\n",
       "│ make it a figure, plot the confusion matrix,  │ make it a figure, plot the confusion matrix, │                  │\n",
       "│ and then return the axes.                     │ and then return the axes.                    │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ So let's outline the plotting steps:          │ So let's outline the plotting steps:         │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 1. Create a figure and axes. Or just use      │ 1. Create a figure and axes. Or just use     │                  │\n",
       "│ plt.subplots to create a figure and axes      │ plt.subplots to create a figure and axes     │                  │\n",
       "│ object.                                       │ object.                                      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 2. Use sns.heatmap to plot the confusion      │ 2. Use sns.heatmap to plot the confusion     │                  │\n",
       "│ matrix. But wait, the confusion matrix is a   │ matrix. But wait, the confusion matrix is a  │                  │\n",
       "│ numpy array. However, the heatmap function    │ numpy array. However, the heatmap function   │                  │\n",
       "│ can take the data, and I can set parameters   │ can take the data, and I can set parameters  │                  │\n",
       "│ like annot=True to show the numbers, and fmt  │ like annot=True to show the numbers, and fmt │                  │\n",
       "│ to format the numbers as integers.            │ to format the numbers as integers.           │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 3. Set the x-axis labels as 'Predicted' and   │ 3. Set the x-axis labels as 'Predicted' and  │                  │\n",
       "│ the y-axis as 'Actual'.                       │ the y-axis as 'Actual'.                      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 4. Also, set the title of the plot.           │ 4. Also, set the title of the plot.          │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ But wait, how do I get the axes object?       │ But wait, how do I get the axes object?      │                  │\n",
       "│ Because when I create the figure and axes, I  │ Because when I create the figure and axes, I │                  │\n",
       "│ can plot on the axes and then return them.    │ can plot on the axes and then return them.   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Alternatively, I can create a new figure,     │ Alternatively, I can create a new figure,    │                  │\n",
       "│ plot on it, and then return the axes.         │ plot on it, and then return the axes.        │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ So putting it all together:                   │ So putting it all together:                  │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Split the data into training and testing    │ - Split the data into training and testing   │                  │\n",
       "│ sets.                                         │ sets.                                        │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Train the logistic regression model.        │ - Train the logistic regression model.       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Predict on the test set.                    │ - Predict on the test set.                   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Compute the confusion matrix.               │ - Compute the confusion matrix.              │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Create a plot with a heatmap.               │ - Create a plot with a heatmap.              │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Now, let's think about possible issues.       │ Now, let's think about possible issues.      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ What if the target has more than two classes? │ What if the target has more than two         │                  │\n",
       "│ The confusion matrix will handle it, but the  │ classes? The confusion matrix will handle    │                  │\n",
       "│ logistic regression model is for binary       │ it, but the logistic regression model is for │                  │\n",
       "│ classification. So the target should be       │ binary classification. So the target should  │                  │\n",
       "│ binary. So I guess the function assumes that  │ be binary. So I guess the function assumes   │                  │\n",
       "│ the target is binary.                         │ that the target is binary.                   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Another thing: when using train_test_split,   │ Another thing: when using train_test_split,  │                  │\n",
       "│ the test size is                              │ the test size is                             │                  │\n",
       "│ default&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistan… │ default&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assista… │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Ensure all required dependencies are          │ Consider switching to `random.randrange` if  │                  │\n",
       "│ installed, including joblib.&lt;|eot_id|&gt;        │ the correct solution uses it&lt;|eot_id|&gt;       │                  │\n",
       "└───────────────────────────────────────────────┴──────────────────────────────────────────────┴──────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mchosen_text                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrejected_text                               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlogits          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ <|begin_of_text|><|start_header_id|>system<|… │ <|begin_of_text|><|start_header_id|>system<… │ [0.4907, 0.5093] │\n",
       "│                                               │                                              │                  │\n",
       "│ Cutting Knowledge Date: December 2023         │ Cutting Knowledge Date: December 2023        │                  │\n",
       "│ Today Date: 07 May 2025                       │ Today Date: 07 May 2025                      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ <|eot_id|><|start_header_id|>user<|end_heade… │ <|eot_id|><|start_header_id|>user<|end_head… │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Give me an advice to the problem and answer   │ Give me an advice to the problem and answer  │                  │\n",
       "│ below;                                        │ below;                                       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Problem:Train a logistic regression model on  │ Problem:Train a logistic regression model on │                  │\n",
       "│ one feature and evaluate its performance      │ one feature and evaluate its performance     │                  │\n",
       "│ using a confusion matrix plot. The function   │ using a confusion matrix plot. The function  │                  │\n",
       "│ takes a feature and a target series, splits   │ takes a feature and a target series, splits  │                  │\n",
       "│ them into training and testing sets, trains   │ them into training and testing sets, trains  │                  │\n",
       "│ the logistic regression model, predicts the   │ the logistic regression model, predicts the  │                  │\n",
       "│ target for the test set, and plots the        │ target for the test set, and plots the       │                  │\n",
       "│ confusion matrix.                             │ confusion matrix.                            │                  │\n",
       "│ The function should output with:              │ The function should output with:             │                  │\n",
       "│     (np.ndarray, plt.Axes): A tuple           │     (np.ndarray, plt.Axes): A tuple          │                  │\n",
       "│ containing the confusion matrix and the       │ containing the confusion matrix and the      │                  │\n",
       "│ matplotlib Axes object of the confusion       │ matplotlib Axes object of the confusion      │                  │\n",
       "│ matrix plot.                                  │ matrix plot.                                 │                  │\n",
       "│ You should write self-contained code starting │ You should write self-contained code         │                  │\n",
       "│ with:                                         │ starting with:                               │                  │\n",
       "│ ```                                           │ ```                                          │                  │\n",
       "│ import pandas as pd                           │ import pandas as pd                          │                  │\n",
       "│ from sklearn.model_selection import           │ from sklearn.model_selection import          │                  │\n",
       "│ train_test_split                              │ train_test_split                             │                  │\n",
       "│ from sklearn.linear_model import              │ from sklearn.linear_model import             │                  │\n",
       "│ LogisticRegression                            │ LogisticRegression                           │                  │\n",
       "│ from sklearn.metrics import confusion_matrix  │ from sklearn.metrics import confusion_matrix │                  │\n",
       "│ import numpy as np                            │ import numpy as np                           │                  │\n",
       "│ import matplotlib.pyplot as plt               │ import matplotlib.pyplot as plt              │                  │\n",
       "│ def task_func(feature: pd.Series, target:     │ def task_func(feature: pd.Series, target:    │                  │\n",
       "│ pd.Series) -> (np.ndarray, plt.Axes):         │ pd.Series) -> (np.ndarray, plt.Axes):        │                  │\n",
       "│ ```                                           │ ```                                          │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Answer:<think>                                │ Answer:<think>                               │                  │\n",
       "│ Okay, I need to write a Python function       │ Okay, I need to write a Python function      │                  │\n",
       "│ called task_func that takes a feature and a   │ called task_func that takes a feature and a  │                  │\n",
       "│ target as inputs and returns a tuple          │ target as inputs and returns a tuple         │                  │\n",
       "│ containing a confusion matrix and a           │ containing a confusion matrix and a          │                  │\n",
       "│ matplotlib Axes object. The function should   │ matplotlib Axes object. The function should  │                  │\n",
       "│ train a logistic regression model on one      │ train a logistic regression model on one     │                  │\n",
       "│ feature and evaluate its performance using a  │ feature and evaluate its performance using a │                  │\n",
       "│ confusion matrix plot.                        │ confusion matrix plot.                       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ First, I should think about the steps         │ First, I should think about the steps        │                  │\n",
       "│ involved. Let's break it down.                │ involved. Let's break it down.               │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ The function starts by importing necessary    │ The function starts by importing necessary   │                  │\n",
       "│ libraries, which are already provided. So I   │ libraries, which are already provided. So I  │                  │\n",
       "│ don't need to worry about that.               │ don't need to worry about that.              │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ The first step is to split the feature and    │ The first step is to split the feature and   │                  │\n",
       "│ target into training and testing sets. I      │ target into training and testing sets. I     │                  │\n",
       "│ remember that the train_test_split function   │ remember that the train_test_split function  │                  │\n",
       "│ from sklearn can do this. So I'll use that.   │ from sklearn can do this. So I'll use that.  │                  │\n",
       "│ The feature is a pandas Series, so I need to  │ The feature is a pandas Series, so I need to │                  │\n",
       "│ convert it into a DataFrame or maybe just an  │ convert it into a DataFrame or maybe just an │                  │\n",
       "│ array? Wait, no. Wait, the feature is a       │ array? Wait, no. Wait, the feature is a      │                  │\n",
       "│ single feature, right? So when I split, I can │ single feature, right? So when I split, I    │                  │\n",
       "│ pass the feature as X and target as y.        │ can pass the feature as X and target as y.   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Wait, but train_test_split expects X and y to │ Wait, but train_test_split expects X and y   │                  │\n",
       "│ be arrays. So for the feature, perhaps I      │ to be arrays. So for the feature, perhaps I  │                  │\n",
       "│ should convert the pd.Series into a numpy     │ should convert the pd.Series into a numpy    │                  │\n",
       "│ array. Because when I pass a Series to        │ array. Because when I pass a Series to       │                  │\n",
       "│ train_test_split, it might work, but          │ train_test_split, it might work, but         │                  │\n",
       "│ sometimes it's better to convert it to a      │ sometimes it's better to convert it to a     │                  │\n",
       "│ numpy array. Alternatively, I can reshape it  │ numpy array. Alternatively, I can reshape it │                  │\n",
       "│ into a 2D array since scikit-learn expects 2D │ into a 2D array since scikit-learn expects   │                  │\n",
       "│ for features.                                 │ 2D for features.                             │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Wait, the feature is a single feature, so     │ Wait, the feature is a single feature, so    │                  │\n",
       "│ it's a 1D array. So when I pass it to         │ it's a 1D array. So when I pass it to        │                  │\n",
       "│ train_test_split, I need to make sure it's in │ train_test_split, I need to make sure it's   │                  │\n",
       "│ the correct shape. So perhaps I should        │ in the correct shape. So perhaps I should    │                  │\n",
       "│ reshape it into a 2D array with shape         │ reshape it into a 2D array with shape        │                  │\n",
       "│ (n_samples, 1). Because LogisticRegression    │ (n_samples, 1). Because LogisticRegression   │                  │\n",
       "│ can handle that.                              │ can handle that.                             │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ So, for X, I'll take                          │ So, for X, I'll take                         │                  │\n",
       "│ feature.values.reshape(-1, 1), and y is       │ feature.values.reshape(-1, 1), and y is      │                  │\n",
       "│ target.values.                                │ target.values.                               │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Then, split them into training and testing    │ Then, split them into training and testing   │                  │\n",
       "│ sets. I can set a random state for            │ sets. I can set a random state for           │                  │\n",
       "│ reproducibility, but it's not specified, so   │ reproducibility, but it's not specified, so  │                  │\n",
       "│ maybe I can leave it out or set it to a       │ maybe I can leave it out or set it to a      │                  │\n",
       "│ specific value like 42 for consistency.       │ specific value like 42 for consistency.      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Next, I need to train the logistic regression │ Next, I need to train the logistic           │                  │\n",
       "│ model. So I'll create an instance of          │ regression model. So I'll create an instance │                  │\n",
       "│ LogisticRegression. The default parameters    │ of LogisticRegression. The default           │                  │\n",
       "│ should be fine, but sometimes it's good to    │ parameters should be fine, but sometimes     │                  │\n",
       "│ set the solver, like 'lbfgs'. But I think the │ it's good to set the solver, like 'lbfgs'.   │                  │\n",
       "│ default is okay.                              │ But I think the default is okay.             │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Then, fit the model on the training data. So  │ Then, fit the model on the training data. So │                  │\n",
       "│ model.fit(X_train, y_train).                  │ model.fit(X_train, y_train).                 │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ After training, I need to predict the target  │ After training, I need to predict the target │                  │\n",
       "│ for the test set. So y_pred =                 │ for the test set. So y_pred =                │                  │\n",
       "│ model.predict(X_test).                        │ model.predict(X_test).                       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Then, compute the confusion matrix using      │ Then, compute the confusion matrix using     │                  │\n",
       "│ confusion_matrix from sklearn.metrics. So cm  │ confusion_matrix from sklearn.metrics. So cm │                  │\n",
       "│ = confusion_matrix(y_test, y_pred).           │ = confusion_matrix(y_test, y_pred).          │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Now, I need to plot the confusion matrix. How │ Now, I need to plot the confusion matrix.    │                  │\n",
       "│ do I do that? I remember that a heatmap is a  │ How do I do that? I remember that a heatmap  │                  │\n",
       "│ good way to visualize it. So I can use        │ is a good way to visualize it. So I can use  │                  │\n",
       "│ matplotlib's imshow function, or perhaps use  │ matplotlib's imshow function, or perhaps use │                  │\n",
       "│ seaborn's heatmap for better aesthetics.      │ seaborn's heatmap for better aesthetics.     │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Wait, but the function needs to return the    │ Wait, but the function needs to return the   │                  │\n",
       "│ confusion matrix as a numpy array and the     │ confusion matrix as a numpy array and the    │                  │\n",
       "│ Axes object. So I'll need to create a plot,   │ Axes object. So I'll need to create a plot,  │                  │\n",
       "│ make it a figure, plot the confusion matrix,  │ make it a figure, plot the confusion matrix, │                  │\n",
       "│ and then return the axes.                     │ and then return the axes.                    │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ So let's outline the plotting steps:          │ So let's outline the plotting steps:         │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 1. Create a figure and axes. Or just use      │ 1. Create a figure and axes. Or just use     │                  │\n",
       "│ plt.subplots to create a figure and axes      │ plt.subplots to create a figure and axes     │                  │\n",
       "│ object.                                       │ object.                                      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 2. Use sns.heatmap to plot the confusion      │ 2. Use sns.heatmap to plot the confusion     │                  │\n",
       "│ matrix. But wait, the confusion matrix is a   │ matrix. But wait, the confusion matrix is a  │                  │\n",
       "│ numpy array. However, the heatmap function    │ numpy array. However, the heatmap function   │                  │\n",
       "│ can take the data, and I can set parameters   │ can take the data, and I can set parameters  │                  │\n",
       "│ like annot=True to show the numbers, and fmt  │ like annot=True to show the numbers, and fmt │                  │\n",
       "│ to format the numbers as integers.            │ to format the numbers as integers.           │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 3. Set the x-axis labels as 'Predicted' and   │ 3. Set the x-axis labels as 'Predicted' and  │                  │\n",
       "│ the y-axis as 'Actual'.                       │ the y-axis as 'Actual'.                      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 4. Also, set the title of the plot.           │ 4. Also, set the title of the plot.          │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ But wait, how do I get the axes object?       │ But wait, how do I get the axes object?      │                  │\n",
       "│ Because when I create the figure and axes, I  │ Because when I create the figure and axes, I │                  │\n",
       "│ can plot on the axes and then return them.    │ can plot on the axes and then return them.   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Alternatively, I can create a new figure,     │ Alternatively, I can create a new figure,    │                  │\n",
       "│ plot on it, and then return the axes.         │ plot on it, and then return the axes.        │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ So putting it all together:                   │ So putting it all together:                  │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Split the data into training and testing    │ - Split the data into training and testing   │                  │\n",
       "│ sets.                                         │ sets.                                        │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Train the logistic regression model.        │ - Train the logistic regression model.       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Predict on the test set.                    │ - Predict on the test set.                   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Compute the confusion matrix.               │ - Compute the confusion matrix.              │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Create a plot with a heatmap.               │ - Create a plot with a heatmap.              │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Now, let's think about possible issues.       │ Now, let's think about possible issues.      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ What if the target has more than two classes? │ What if the target has more than two         │                  │\n",
       "│ The confusion matrix will handle it, but the  │ classes? The confusion matrix will handle    │                  │\n",
       "│ logistic regression model is for binary       │ it, but the logistic regression model is for │                  │\n",
       "│ classification. So the target should be       │ binary classification. So the target should  │                  │\n",
       "│ binary. So I guess the function assumes that  │ be binary. So I guess the function assumes   │                  │\n",
       "│ the target is binary.                         │ that the target is binary.                   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Another thing: when using train_test_split,   │ Another thing: when using train_test_split,  │                  │\n",
       "│ the test size is                              │ the test size is                             │                  │\n",
       "│ default<|eot_id|><|start_header_id|>assistan… │ default<|eot_id|><|start_header_id|>assista… │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Ensure all required dependencies are          │ Consider switching to `random.randrange` if  │                  │\n",
       "│ installed, including joblib.<|eot_id|>        │ the correct solution uses it<|eot_id|>       │                  │\n",
       "└───────────────────────────────────────────────┴──────────────────────────────────────────────┴──────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='2205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  24/2205 06:03 < 10:01:09, 0.06 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.339076</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.230500</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.627400</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.531700</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.303600</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.242800</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.193100</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.064900</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.230300</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.090900</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import RewardConfig\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "batch_size_per_device = 4\n",
    "eval_batch_size_per_device = 4\n",
    "\n",
    "training_args = RewardConfig(\n",
    "    output_dir=model_save_dir,\n",
    "    per_device_train_batch_size=batch_size_per_device,\n",
    "    per_device_eval_batch_size=eval_batch_size_per_device,\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    eval_on_start=True,\n",
    "    save_steps=20,\n",
    "    logging_steps=1,\n",
    "    num_train_epochs = 3,\n",
    "    report_to=None,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    target_modules=[\"k_proj\",\"q_proj\",\"o_proj\", \"v_proj\",\"down_proj\",\"gate_proj\",\"up_proj\",],\n",
    "    layers_to_transform=[25,26,27],\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "rmtrainer.train(formatted_dataset, training_args = training_args, peft_config = peft_config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
