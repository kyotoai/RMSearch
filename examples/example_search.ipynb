{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff024e43-f629-4b68-bf88-73b20b235e02",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "755c8fdb-2d98-4806-badc-64f1067c236b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /workspace/RMSearch1-3\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from rmsearch==0.1.0) (4.51.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from rmsearch==0.1.0) (3.6.0)\n",
      "Requirement already satisfied: trl==0.14.0 in /usr/local/lib/python3.11/dist-packages (from rmsearch==0.1.0) (0.14.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from rmsearch==0.1.0) (0.15.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from rmsearch==0.1.0) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from rmsearch==0.1.0) (3.10.1)\n",
      "Requirement already satisfied: vllm==0.6.5 in /usr/local/lib/python3.11/dist-packages (from rmsearch==0.1.0) (0.6.5)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (from rmsearch==0.1.0) (0.4.3)\n",
      "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (from rmsearch==0.1.0) (4.1.0)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.14.0->rmsearch==0.1.0) (1.6.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl==0.14.0->rmsearch==0.1.0) (14.0.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (6.0.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (1.26.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (4.67.1)\n",
      "Requirement already satisfied: blake3 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (1.0.4)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (9.0.0)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.21.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (6.30.2)\n",
      "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.115.12)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (3.11.18)\n",
      "Requirement already satisfied: openai>=1.45.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (1.77.0)\n",
      "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.34.2)\n",
      "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (2.11.4)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (11.2.1)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.21.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (7.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.9 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.10.11)\n",
      "Requirement already satisfied: outlines==0.1.11 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.1.11)\n",
      "Requirement already satisfied: xgrammar>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.1.18)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (4.13.2)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (3.18.0)\n",
      "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.2.1.1.post5)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (24.0.1)\n",
      "Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.19.0)\n",
      "Requirement already satisfied: gguf==0.10.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.10.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (4.6.4)\n",
      "Requirement already satisfied: mistral_common>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.0->vllm==0.6.5->rmsearch==0.1.0) (1.5.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.8.1)\n",
      "Requirement already satisfied: compressed-tensors==0.8.1 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.8.1)\n",
      "Requirement already satisfied: depyf==0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.18.0)\n",
      "Requirement already satisfied: ray>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (2.46.0)\n",
      "Requirement already satisfied: nvidia-ml-py>=12.560.30 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (12.575.51)\n",
      "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (2.5.1)\n",
      "Requirement already satisfied: torchvision==0.20.1 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.20.1)\n",
      "Requirement already satisfied: xformers==0.0.28.post3 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.5->rmsearch==0.1.0) (0.0.28.post3)\n",
      "Requirement already satisfied: astor in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm==0.6.5->rmsearch==0.1.0) (0.8.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm==0.6.5->rmsearch==0.1.0) (0.3.8)\n",
      "Requirement already satisfied: interegular in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.6.5->rmsearch==0.1.0) (0.3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.6.5->rmsearch==0.1.0) (3.1.3)\n",
      "Requirement already satisfied: lark in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.6.5->rmsearch==0.1.0) (1.2.2)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.6.5->rmsearch==0.1.0) (1.6.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.6.5->rmsearch==0.1.0) (3.1.1)\n",
      "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.6.5->rmsearch==0.1.0) (5.6.3)\n",
      "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.6.5->rmsearch==0.1.0) (0.35.1)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.6.5->rmsearch==0.1.0) (4.23.0)\n",
      "Requirement already satisfied: pycountry in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.6.5->rmsearch==0.1.0) (24.6.1)\n",
      "Requirement already satisfied: airportsdata in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.6.5->rmsearch==0.1.0) (20250224)\n",
      "Requirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm==0.6.5->rmsearch==0.1.0) (0.1.26)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->vllm==0.6.5->rmsearch==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->rmsearch==0.1.0) (20.0.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->rmsearch==0.1.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->rmsearch==0.1.0) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->rmsearch==0.1.0) (0.31.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->rmsearch==0.1.0) (24.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->rmsearch==0.1.0) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->rmsearch==0.1.0) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->rmsearch==0.1.0) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->rmsearch==0.1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->rmsearch==0.1.0) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->rmsearch==0.1.0) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->rmsearch==0.1.0) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->rmsearch==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->rmsearch==0.1.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->rmsearch==0.1.0) (2025.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers->rmsearch==0.1.0) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers->rmsearch==0.1.0) (1.15.2)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm==0.6.5->rmsearch==0.1.0) (0.46.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.5->rmsearch==0.1.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.5->rmsearch==0.1.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.5->rmsearch==0.1.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.5->rmsearch==0.1.0) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.5->rmsearch==0.1.0) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.5->rmsearch==0.1.0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.5->rmsearch==0.1.0) (1.20.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets->rmsearch==0.1.0) (1.1.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.0->vllm==0.6.5->rmsearch==0.1.0) (4.11.0.86)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.45.0->vllm==0.6.5->rmsearch==0.1.0) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.45.0->vllm==0.6.5->rmsearch==0.1.0) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.45.0->vllm==0.6.5->rmsearch==0.1.0) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.45.0->vllm==0.6.5->rmsearch==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.45.0->vllm==0.6.5->rmsearch==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm==0.6.5->rmsearch==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm==0.6.5->rmsearch==0.1.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm==0.6.5->rmsearch==0.1.0) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->rmsearch==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.6.5->rmsearch==0.1.0) (8.1.8)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.6.5->rmsearch==0.1.0) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm==0.6.5->rmsearch==0.1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm==0.6.5->rmsearch==0.1.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm==0.6.5->rmsearch==0.1.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm==0.6.5->rmsearch==0.1.0) (2024.8.30)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from xgrammar>=0.1.6->vllm==0.6.5->rmsearch==0.1.0) (1.11.1.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.14.0->rmsearch==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.14.0->rmsearch==0.1.0) (2.18.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers->rmsearch==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers->rmsearch==0.1.0) (3.6.0)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.6.5->rmsearch==0.1.0) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.6.5->rmsearch==0.1.0) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.6.5->rmsearch==0.1.0) (1.1.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.6.5->rmsearch==0.1.0) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.6.5->rmsearch==0.1.0) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.6.5->rmsearch==0.1.0) (15.0.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.45.0->vllm==0.6.5->rmsearch==0.1.0) (1.0.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.6.5->rmsearch==0.1.0) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.6.5->rmsearch==0.1.0) (0.20.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.14.0->rmsearch==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm==0.6.5->rmsearch==0.1.0) (2.1.5)\n",
      "Building wheels for collected packages: rmsearch\n",
      "  Building wheel for rmsearch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rmsearch: filename=rmsearch-0.1.0-py3-none-any.whl size=7743 sha256=6d14c0bf08be8ecb214849d1f4ace406abd5405dd066d23540f214b4b4bd4bf5\n",
      "  Stored in directory: /root/.cache/pip/wheels/9d/13/b2/fabbc0de47e01fe6408c2f07683041d9b31bf3ea04516d2a0d\n",
      "Successfully built rmsearch\n",
      "Installing collected packages: rmsearch\n",
      "  Attempting uninstall: rmsearch\n",
      "    Found existing installation: rmsearch 0.1.0\n",
      "    Uninstalling rmsearch-0.1.0:\n",
      "      Successfully uninstalled rmsearch-0.1.0\n",
      "Successfully installed rmsearch-0.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install /workspace/RMSearch1-3/\n",
    "# or\n",
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeab20a-f36e-4915-a06a-6a2a22d88a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following commands in terminal\n",
    "'''\n",
    "cd /workspace\n",
    "pip install \"huggingface_hub[hf_transfer]\"\n",
    "pip install hf_transfer\n",
    "HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download Ray2333/GRM-Llama3.2-3B-rewardmodel-ft --local-dir ./llama3b-rm/\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807277fd-70a0-48ae-aedc-d0929691741f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Search Keys from Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1837433-c24b-4147-968e-ab6229f60d10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TypeIs' from 'typing_extensions' (/usr/local/lib/python3.11/dist-packages/typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrmsearch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Search\n\u001b[1;32m      2\u001b[0m search \u001b[38;5;241m=\u001b[39m Search(model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspace/llama3b-rm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m         tensor_parallel_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      4\u001b[0m         pipeline_parallel_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,)\n",
      "File \u001b[0;32m/workspace/RMSearch1-3/rmsearch/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrmtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RMTrainer\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrmsearch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Search\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSearch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMTrainer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/workspace/RMSearch1-3/rmsearch/rmsearch.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscheduling_strategies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PlacementGroupSchedulingStrategy\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM, PoolingParams, SamplingParams, AsyncEngineArgs, AsyncLLMEngine\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Version(ray\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m Version(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.22.0\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRay version must be at least 2.22.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"vLLM: a high-throughput and memory-efficient inference engine for LLMs\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marg_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncEngineArgs, EngineArgs\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_llm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncLLMEngine\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMEngine\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01menvs\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (CacheConfig, CompilationConfig, ConfigFormat,\n\u001b[1;32m     12\u001b[0m                          DecodingConfig, DeviceConfig, HfOverrides,\n\u001b[1;32m     13\u001b[0m                          KVTransferConfig, LoadConfig, LoadFormat, LoRAConfig,\n\u001b[1;32m     14\u001b[0m                          ModelConfig, ObservabilityConfig, ParallelConfig,\n\u001b[1;32m     15\u001b[0m                          PoolerConfig, PromptAdapterConfig, SchedulerConfig,\n\u001b[1;32m     16\u001b[0m                          SpeculativeConfig, TaskOption, TokenizerPoolConfig,\n\u001b[1;32m     17\u001b[0m                          VllmConfig)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExecutorBase\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/config.py:16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (TYPE_CHECKING, Any, Callable, ClassVar, Counter, Dict,\n\u001b[1;32m     12\u001b[0m                     Final, List, Literal, Mapping, Optional, Set, Tuple, Type,\n\u001b[1;32m     13\u001b[0m                     Union)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field, PrivateAttr\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01menvs\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/__init__.py:413\u001b[0m\n\u001b[1;32m    242\u001b[0m _dynamic_imports: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdict[str, tuple[str, str]]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataclasses\u001b[39m\u001b[38;5;124m'\u001b[39m: (__spec__\u001b[38;5;241m.\u001b[39mparent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__module__\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# functional validators\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerateSchema\u001b[39m\u001b[38;5;124m'\u001b[39m: (__spec__\u001b[38;5;241m.\u001b[39mparent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m._internal._generate_schema\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    410\u001b[0m }\n\u001b[1;32m    411\u001b[0m _deprecated_dynamic_imports \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFieldValidationInfo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerateSchema\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m--> 413\u001b[0m _getattr_migration \u001b[38;5;241m=\u001b[39m \u001b[43mgetattr_migration\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(attr_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attr_name \u001b[38;5;129;01min\u001b[39;00m _deprecated_dynamic_imports:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/_migration.py:260\u001b[0m, in \u001b[0;36mgetattr_migration\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implement PEP 562 for objects that were either moved or removed on the migration\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03mto V2.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    A callable that will raise an error if the object is not found.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# This avoids circular import with errors.py.\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PydanticImportError\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    263\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise an error if the object is not found, or warn if it was moved.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    In case it was moved, it still returns the object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m        The object.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/errors.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, ClassVar, Literal\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Self\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_inspection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintrospection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Qualifier\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _repr\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_migration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getattr_migration\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/typing_inspection/introspection.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Literal, NamedTuple, cast\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypeAlias, assert_never, get_args, get_origin\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m typing_objects\n\u001b[1;32m     16\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnnotationSource\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mForbiddenQualifier\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_union_origin\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/typing_inspection/typing_objects.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Final\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LiteralString, TypeAliasType, TypeIs, deprecated\n\u001b[1;32m     21\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEPRECATED_ALIASES\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoneType\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_unpack\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     55\u001b[0m _IS_PY310 \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TypeIs' from 'typing_extensions' (/usr/local/lib/python3.11/dist-packages/typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "from rmsearch import Search\n",
    "search = Search(model_name = \"/workspace/llama3b-rm\",\n",
    "        tensor_parallel_size = 1,\n",
    "        pipeline_parallel_size = 1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe3c6c1-4fa2-47a7-a9d1-331b493a1a62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "queries = [\"How to make LLM?\", \"What's the capital of Japan?\"] * 5\n",
    "keys = [\"LLM is Large Language Model which can be made ...\"*7, \"Japanese capital is ...\"*7] * 5\n",
    "output = await search(queries, keys)\n",
    "\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "458bd83c-804f-4bbf-9308-1899bd6adad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'How to make LLM?',\n",
       "  'query_id': 0,\n",
       "  'keys': [{'key_id': 8,\n",
       "    'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 6, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 0, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 4, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 2, 'key': 'LLM is Large Language Model which can be made ...'}]},\n",
       " {'query': \"What's the capital of Japan?\",\n",
       "  'query_id': 1,\n",
       "  'keys': [{'key_id': 5, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 9, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 7, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 1, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 3, 'key': 'Japanese capital is ...'}]},\n",
       " {'query': 'How to make LLM?',\n",
       "  'query_id': 2,\n",
       "  'keys': [{'key_id': 8,\n",
       "    'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 6, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 0, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 4, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 2, 'key': 'LLM is Large Language Model which can be made ...'}]},\n",
       " {'query': \"What's the capital of Japan?\",\n",
       "  'query_id': 3,\n",
       "  'keys': [{'key_id': 5, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 9, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 7, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 1, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 3, 'key': 'Japanese capital is ...'}]},\n",
       " {'query': 'How to make LLM?',\n",
       "  'query_id': 4,\n",
       "  'keys': [{'key_id': 8,\n",
       "    'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 6, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 0, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 4, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 2, 'key': 'LLM is Large Language Model which can be made ...'}]},\n",
       " {'query': \"What's the capital of Japan?\",\n",
       "  'query_id': 5,\n",
       "  'keys': [{'key_id': 5, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 9, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 7, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 1, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 3, 'key': 'Japanese capital is ...'}]},\n",
       " {'query': 'How to make LLM?',\n",
       "  'query_id': 6,\n",
       "  'keys': [{'key_id': 8,\n",
       "    'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 6, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 0, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 4, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 2, 'key': 'LLM is Large Language Model which can be made ...'}]},\n",
       " {'query': \"What's the capital of Japan?\",\n",
       "  'query_id': 7,\n",
       "  'keys': [{'key_id': 5, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 9, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 7, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 1, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 3, 'key': 'Japanese capital is ...'}]},\n",
       " {'query': 'How to make LLM?',\n",
       "  'query_id': 8,\n",
       "  'keys': [{'key_id': 8,\n",
       "    'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 6, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 0, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 4, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 2, 'key': 'LLM is Large Language Model which can be made ...'}]},\n",
       " {'query': \"What's the capital of Japan?\",\n",
       "  'query_id': 9,\n",
       "  'keys': [{'key_id': 5, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 9, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 7, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 1, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 3, 'key': 'Japanese capital is ...'}]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42637ff3-54d0-419e-9a51-a26a4da7b2e8",
   "metadata": {},
   "source": [
    "# Train Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34af2fe2-4b78-4220-a6c3-f2afb26ada19",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = \"/workspace/exp12\"\n",
    "model_save_dir = f\"{exp_dir}/model1\"\n",
    "model_name = \"/workspace/llama3b-rm\"\n",
    "data_save_path = \"/workspace/bigcodebench3.json\"\n",
    "advice_save_path = \"/workspace/code-log5-corr1.json\"\n",
    "data_dict_save_path = f\"{exp_dir}/data_dict.json\"\n",
    "dataset_list_save_path = f\"{exp_dir}/dataset_list.json\"\n",
    "#num_advice_per_batch = 13  # total number of advice including both chosen and rejected per 1 batch\n",
    "avoid_topk = 10 # avoid topk similar advices to each advice list being included in rejected advice\n",
    "\n",
    "import json, os\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f43b006-4c89-419b-8b1e-e4c9dba1022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference\n",
    "# https://medium.com/towards-generative-ai/reward-model-training-2209d1befb5f\n",
    "\n",
    "if not os.path.exists(data_dict_save_path):\n",
    "    \n",
    "    with open(data_save_path) as f:\n",
    "        data_dict = json.load(f)\n",
    "    \n",
    "    task_id=data_dict[\"task_id\"]\n",
    "    complete_prompt=data_dict[\"complete_prompt\"]\n",
    "    instruct_prompt=data_dict[\"instruct_prompt\"]\n",
    "    canonical_solution=data_dict[\"canonical_solution\"]\n",
    "    code_prompt=data_dict[\"code_prompt\"]\n",
    "    test=data_dict[\"test\"]\n",
    "    doc_struct=data_dict[\"doc_struct\"]\n",
    "    num_problems = len(task_id)\n",
    "    \n",
    "    with open(advice_save_path) as f:\n",
    "        log = json.load(f)\n",
    "    \n",
    "    data_dict = {\"problem_id\":[], \"problem\":[], \"advice\":[], \"advice_id\":[], \"output\":[], \"correct\":[], \"log_ids\":[], \"node_state\":[]}\n",
    "    \n",
    "    def search_log(log_dict, log_ids):  # node_id: str\n",
    "        global data_dict\n",
    "        node_id = log_ids[-1]\n",
    "    \n",
    "        if type(log_dict[node_id]) != dict:\n",
    "            return None\n",
    "            \n",
    "        if \"children\" in log_dict[node_id]:\n",
    "            for next_node_id in log_dict[node_id][\"children\"]:\n",
    "                problem_id_str = log_ids[0]\n",
    "                problem_id = int(problem_id_str)\n",
    "                problem = instruct_prompt[problem_id]\n",
    "                advice_list = log_dict[node_id][\"children\"][next_node_id][\"instruction_list\"]\n",
    "    \n",
    "                if \"corrects\" in log_dict[node_id][\"children\"][next_node_id]:  # when quiting the inference after finihing creating advice\n",
    "                    all_correct = True\n",
    "                    all_incorrect = True\n",
    "                    child_num_correct = 0\n",
    "                    child_num_problem = 0\n",
    "                    for advice_id, advice in enumerate(advice_list):\n",
    "                        correct = log_dict[node_id][\"children\"][next_node_id][\"corrects\"][str(advice_id)]\n",
    "                        child_num_problem += 1\n",
    "                        if correct:\n",
    "                            all_incorrect = False\n",
    "                            child_num_correct += 1\n",
    "                        else:\n",
    "                            all_correct = False\n",
    "    \n",
    "                    if child_num_problem == 0: continue\n",
    "    \n",
    "                    for advice_id, advice in enumerate(advice_list):\n",
    "                        correct = log_dict[node_id][\"children\"][next_node_id][\"corrects\"][str(advice_id)]\n",
    "                        #output = log_dict[node_id][\"children\"][next_node_id][\"outputs\"][str(advice_id)]\n",
    "                        output = log_dict[node_id][\"outputs\"][\"0\"]\n",
    "                        data_dict[\"problem_id\"].append(problem_id)\n",
    "                        data_dict[\"problem\"].append(problem)\n",
    "                        data_dict[\"output\"].append(output)\n",
    "                        data_dict[\"advice\"].append(advice)\n",
    "                        data_dict[\"advice_id\"].append(advice_id)\n",
    "                        data_dict[\"correct\"].append(correct)\n",
    "                        data_dict[\"log_ids\"].append(log_ids+[next_node_id])\n",
    "                        data_dict[\"node_state\"].append({\"num_problem\":len(advice_list), \"child_num_correct\":child_num_correct, \"all_correct\":all_correct, \"all_incorrect\":all_incorrect})\n",
    "    \n",
    "                    if all_incorrect:\n",
    "                        search_log(log_dict[node_id][\"children\"], log_ids + [next_node_id])\n",
    "    \n",
    "    problem_ids_with_advice = []\n",
    "    for problem_id_str in log:\n",
    "        if type(log[problem_id_str])==dict:\n",
    "            if \"children\" in log[problem_id_str]:\n",
    "                problem_ids_with_advice.append(int(problem_id_str))\n",
    "                search_log(log, [problem_id_str])\n",
    "    \n",
    "    with open(f\"{exp_dir}/data_dict.json\", \"w\") as f: json.dump(data_dict, f)\n",
    "    with open(f\"{exp_dir}/problem_ids_with_advice.json\", \"w\") as f: json.dump(problem_ids_with_advice, f)\n",
    "\n",
    "else:\n",
    "    with open(f\"{exp_dir}/data_dict.json\") as f: data_dict = json.load(f)\n",
    "    with open(f\"{exp_dir}/problem_ids_with_advice.json\") as f: problem_ids_with_advice = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "820dd6be-ad67-4200-af20-e12cda2bad9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1399/4218478872.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  advice_similarities = torch.load(f\"{exp_dir}/advice_similarities.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if not os.path.exists(f\"{exp_dir}/advice_similarities.pt\"):\n",
    "    # Get similarities between advice so that avoid rejected advice being important for the problems\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sentence_transformers.util import cos_sim\n",
    "    from sentence_transformers.quantization import quantize_embeddings\n",
    "    \n",
    "    # 1. Specify preffered dimensions\n",
    "    dimensions = 512\n",
    "    \n",
    "    # 2. load model\n",
    "    model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim=dimensions).to(\"cuda\")\n",
    "    advices = data_dict[\"advice\"]\n",
    "    # The prompt used for query retrieval tasks:\n",
    "    # query_prompt = 'Represent this sentence for searching relevant passages: '\n",
    "    \n",
    "    # 2. Encode\n",
    "    query_embedding = model.encode(advices, prompt_name=\"query\")\n",
    "    # Equivalent Alternatives:\n",
    "    # query_embedding = model.encode(query_prompt + query)\n",
    "    # query_embedding = model.encode(query, prompt=query_prompt)\n",
    "    \n",
    "    docs_embeddings = model.encode(advices)\n",
    "    \n",
    "    # Optional: Quantize the embeddings\n",
    "    binary_query_embedding = quantize_embeddings(query_embedding, precision=\"ubinary\")\n",
    "    binary_query_embedding = torch.tensor(binary_query_embedding).to(\"cuda\")\n",
    "    binary_docs_embeddings = quantize_embeddings(docs_embeddings, precision=\"ubinary\")\n",
    "    binary_docs_embeddings = torch.tensor(binary_docs_embeddings).to(\"cuda\")\n",
    "    \n",
    "    advice_similarities = cos_sim(query_embedding, docs_embeddings)\n",
    "    torch.save(advice_similarities, f\"{exp_dir}/advice_similarities.pt\")\n",
    "    \n",
    "else:\n",
    "    advice_similarities = torch.load(f\"{exp_dir}/advice_similarities.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03982b90-d188-4f13-b087-a7d17d7d0efc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "if not os.path.exists(dataset_list_save_path):\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    df['id'] = range(len(df))\n",
    "    dataset_list = []\n",
    "    \n",
    "    for problem_id in problem_ids_with_advice:\n",
    "        #problem = instruct_prompt[problem_id]\n",
    "        chosen_df = df[df['problem_id'] == problem_id]\n",
    "        problem = chosen_df['problem'].values[0]\n",
    "        output = chosen_df['output'].values[0]\n",
    "        chosen_ids = chosen_df[\"id\"].tolist()\n",
    "        all_avoid_ids = []\n",
    "        for chosen_id in chosen_ids:\n",
    "            _, avoid_ids = torch.topk(advice_similarities[chosen_id], k=avoid_topk)\n",
    "            avoid_ids = avoid_ids.tolist()\n",
    "            all_avoid_ids += avoid_ids\n",
    "        \n",
    "        rejected_df = df.drop(all_avoid_ids)[df['problem_id'] != problem_id]\n",
    "        num_chosen = len(chosen_df)\n",
    "        num_rejected = num_chosen\n",
    "        if not chosen_df.empty and not rejected_df.empty:\n",
    "            '''\n",
    "            if len_chosen_df<4:\n",
    "                num_chosen = len_chosen_df\n",
    "                num_rejected = num_advice_per_batch - len_chosen_df\n",
    "            else: # For the case there are too many chosen advices\n",
    "                num_chosen = 3\n",
    "                num_rejected = num_advice_per_batch - num_chosen\n",
    "            '''\n",
    "            chosen_rows = chosen_df.sample(n=num_chosen)\n",
    "            rejected_rows = rejected_df.sample(n=num_rejected)\n",
    "            chosen_advice_ids = chosen_rows[\"id\"].tolist()\n",
    "            rejected_advice_ids = rejected_rows[\"id\"].tolist()\n",
    "            #chosen_reject_similarities = advice_similarities[chosen_ids][:, rejected_ids]\n",
    "            for i in range(num_chosen):\n",
    "                chosen_advice = chosen_rows['advice'].values[i]\n",
    "                rejected_advice = rejected_rows['advice'].values[i]\n",
    "                chosen_advice_id = chosen_advice_ids[i]\n",
    "                rejected_advice_id = rejected_advice_ids[i]\n",
    "                \n",
    "                dataset_list.append({\n",
    "                    \"query\":[{'role': 'user', 'content':f\"Give me an advice to the problem and answer below;\\n\\nProblem:{problem}\\n\\nAnswer:{output}\"}],\n",
    "                    \"chosen_key\":[{'role': 'assistant', 'content': f\"{chosen_advice}\"}],\n",
    "                    \"rejected_key\":[{'role': 'assistant', 'content': f\"{rejected_advice}\"}],\n",
    "                    \"problem_id\":problem_id,\n",
    "                    \"problem\":problem,\n",
    "                    \"output\":output,\n",
    "                    \"chosen_advice\":chosen_advice,\n",
    "                    \"rejected_advice\":rejected_advice,\n",
    "                    \"chosen_advice_id\":chosen_advice_id,\n",
    "                    \"rejected_advice_id\":rejected_advice_id,\n",
    "                })\n",
    "        else:\n",
    "            #print(f\"problem {problem_id} doesn't have any advice\")\n",
    "            continue\n",
    "    \n",
    "    #if num_gpu*batch_size_per_device != 1:\n",
    "    #    num_trash = len(dataset_list)%(num_gpu*batch_size_per_device)\n",
    "    #    dataset_list = dataset_list[:-num_trash]\n",
    "    \n",
    "    with open(dataset_list_save_path, \"w\") as f:\n",
    "        json.dump(dataset_list, f)\n",
    "        \n",
    "else:\n",
    "    with open(dataset_list_save_path) as f:\n",
    "        dataset_list = json.load(f)\n",
    "\n",
    "#dataset1 = Dataset.from_list(dataset_list)\n",
    "#dataset1.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e893b8f-386c-47ac-8c28-66bb48b3e204",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0578ac074b514a20b053bbe67cc43ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rmsearch import RMTrainer\n",
    "\n",
    "model_name = \"/workspace/llama3b-rm\"\n",
    "num_gpus = 1\n",
    "\n",
    "rmtrainer = RMTrainer(model_name = model_name, num_gpus = num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da4b14f-34ef-45e5-973b-def999ce1a65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_save_path = f\"{exp_dir}/dataset\"\n",
    "train_ids_save_path = f\"{exp_dir}/train_ids.json\"\n",
    "test_ids_save_path = f\"{exp_dir}/test_ids.json\"\n",
    "test_size = 48\n",
    "\n",
    "formatted_dataset = rmtrainer.prepare_dataset(dataset_list, dataset_save_path, test_size, train_ids_save_path, test_ids_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77839511-8a0b-4052-a4a2-2e3f3fd9ae6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> chosen_text                                   </span>┃<span style=\"font-weight: bold\"> rejected_text                                </span>┃<span style=\"font-weight: bold\"> logits           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ &lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|… │ &lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;… │ [0.4907, 0.5093] │\n",
       "│                                               │                                              │                  │\n",
       "│ Cutting Knowledge Date: December 2023         │ Cutting Knowledge Date: December 2023        │                  │\n",
       "│ Today Date: 07 May 2025                       │ Today Date: 07 May 2025                      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ &lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_heade… │ &lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_head… │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Give me an advice to the problem and answer   │ Give me an advice to the problem and answer  │                  │\n",
       "│ below;                                        │ below;                                       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Problem:Train a logistic regression model on  │ Problem:Train a logistic regression model on │                  │\n",
       "│ one feature and evaluate its performance      │ one feature and evaluate its performance     │                  │\n",
       "│ using a confusion matrix plot. The function   │ using a confusion matrix plot. The function  │                  │\n",
       "│ takes a feature and a target series, splits   │ takes a feature and a target series, splits  │                  │\n",
       "│ them into training and testing sets, trains   │ them into training and testing sets, trains  │                  │\n",
       "│ the logistic regression model, predicts the   │ the logistic regression model, predicts the  │                  │\n",
       "│ target for the test set, and plots the        │ target for the test set, and plots the       │                  │\n",
       "│ confusion matrix.                             │ confusion matrix.                            │                  │\n",
       "│ The function should output with:              │ The function should output with:             │                  │\n",
       "│     (np.ndarray, plt.Axes): A tuple           │     (np.ndarray, plt.Axes): A tuple          │                  │\n",
       "│ containing the confusion matrix and the       │ containing the confusion matrix and the      │                  │\n",
       "│ matplotlib Axes object of the confusion       │ matplotlib Axes object of the confusion      │                  │\n",
       "│ matrix plot.                                  │ matrix plot.                                 │                  │\n",
       "│ You should write self-contained code starting │ You should write self-contained code         │                  │\n",
       "│ with:                                         │ starting with:                               │                  │\n",
       "│ ```                                           │ ```                                          │                  │\n",
       "│ import pandas as pd                           │ import pandas as pd                          │                  │\n",
       "│ from sklearn.model_selection import           │ from sklearn.model_selection import          │                  │\n",
       "│ train_test_split                              │ train_test_split                             │                  │\n",
       "│ from sklearn.linear_model import              │ from sklearn.linear_model import             │                  │\n",
       "│ LogisticRegression                            │ LogisticRegression                           │                  │\n",
       "│ from sklearn.metrics import confusion_matrix  │ from sklearn.metrics import confusion_matrix │                  │\n",
       "│ import numpy as np                            │ import numpy as np                           │                  │\n",
       "│ import matplotlib.pyplot as plt               │ import matplotlib.pyplot as plt              │                  │\n",
       "│ def task_func(feature: pd.Series, target:     │ def task_func(feature: pd.Series, target:    │                  │\n",
       "│ pd.Series) -&gt; (np.ndarray, plt.Axes):         │ pd.Series) -&gt; (np.ndarray, plt.Axes):        │                  │\n",
       "│ ```                                           │ ```                                          │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Answer:&lt;think&gt;                                │ Answer:&lt;think&gt;                               │                  │\n",
       "│ Okay, I need to write a Python function       │ Okay, I need to write a Python function      │                  │\n",
       "│ called task_func that takes a feature and a   │ called task_func that takes a feature and a  │                  │\n",
       "│ target as inputs and returns a tuple          │ target as inputs and returns a tuple         │                  │\n",
       "│ containing a confusion matrix and a           │ containing a confusion matrix and a          │                  │\n",
       "│ matplotlib Axes object. The function should   │ matplotlib Axes object. The function should  │                  │\n",
       "│ train a logistic regression model on one      │ train a logistic regression model on one     │                  │\n",
       "│ feature and evaluate its performance using a  │ feature and evaluate its performance using a │                  │\n",
       "│ confusion matrix plot.                        │ confusion matrix plot.                       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ First, I should think about the steps         │ First, I should think about the steps        │                  │\n",
       "│ involved. Let's break it down.                │ involved. Let's break it down.               │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ The function starts by importing necessary    │ The function starts by importing necessary   │                  │\n",
       "│ libraries, which are already provided. So I   │ libraries, which are already provided. So I  │                  │\n",
       "│ don't need to worry about that.               │ don't need to worry about that.              │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ The first step is to split the feature and    │ The first step is to split the feature and   │                  │\n",
       "│ target into training and testing sets. I      │ target into training and testing sets. I     │                  │\n",
       "│ remember that the train_test_split function   │ remember that the train_test_split function  │                  │\n",
       "│ from sklearn can do this. So I'll use that.   │ from sklearn can do this. So I'll use that.  │                  │\n",
       "│ The feature is a pandas Series, so I need to  │ The feature is a pandas Series, so I need to │                  │\n",
       "│ convert it into a DataFrame or maybe just an  │ convert it into a DataFrame or maybe just an │                  │\n",
       "│ array? Wait, no. Wait, the feature is a       │ array? Wait, no. Wait, the feature is a      │                  │\n",
       "│ single feature, right? So when I split, I can │ single feature, right? So when I split, I    │                  │\n",
       "│ pass the feature as X and target as y.        │ can pass the feature as X and target as y.   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Wait, but train_test_split expects X and y to │ Wait, but train_test_split expects X and y   │                  │\n",
       "│ be arrays. So for the feature, perhaps I      │ to be arrays. So for the feature, perhaps I  │                  │\n",
       "│ should convert the pd.Series into a numpy     │ should convert the pd.Series into a numpy    │                  │\n",
       "│ array. Because when I pass a Series to        │ array. Because when I pass a Series to       │                  │\n",
       "│ train_test_split, it might work, but          │ train_test_split, it might work, but         │                  │\n",
       "│ sometimes it's better to convert it to a      │ sometimes it's better to convert it to a     │                  │\n",
       "│ numpy array. Alternatively, I can reshape it  │ numpy array. Alternatively, I can reshape it │                  │\n",
       "│ into a 2D array since scikit-learn expects 2D │ into a 2D array since scikit-learn expects   │                  │\n",
       "│ for features.                                 │ 2D for features.                             │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Wait, the feature is a single feature, so     │ Wait, the feature is a single feature, so    │                  │\n",
       "│ it's a 1D array. So when I pass it to         │ it's a 1D array. So when I pass it to        │                  │\n",
       "│ train_test_split, I need to make sure it's in │ train_test_split, I need to make sure it's   │                  │\n",
       "│ the correct shape. So perhaps I should        │ in the correct shape. So perhaps I should    │                  │\n",
       "│ reshape it into a 2D array with shape         │ reshape it into a 2D array with shape        │                  │\n",
       "│ (n_samples, 1). Because LogisticRegression    │ (n_samples, 1). Because LogisticRegression   │                  │\n",
       "│ can handle that.                              │ can handle that.                             │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ So, for X, I'll take                          │ So, for X, I'll take                         │                  │\n",
       "│ feature.values.reshape(-1, 1), and y is       │ feature.values.reshape(-1, 1), and y is      │                  │\n",
       "│ target.values.                                │ target.values.                               │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Then, split them into training and testing    │ Then, split them into training and testing   │                  │\n",
       "│ sets. I can set a random state for            │ sets. I can set a random state for           │                  │\n",
       "│ reproducibility, but it's not specified, so   │ reproducibility, but it's not specified, so  │                  │\n",
       "│ maybe I can leave it out or set it to a       │ maybe I can leave it out or set it to a      │                  │\n",
       "│ specific value like 42 for consistency.       │ specific value like 42 for consistency.      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Next, I need to train the logistic regression │ Next, I need to train the logistic           │                  │\n",
       "│ model. So I'll create an instance of          │ regression model. So I'll create an instance │                  │\n",
       "│ LogisticRegression. The default parameters    │ of LogisticRegression. The default           │                  │\n",
       "│ should be fine, but sometimes it's good to    │ parameters should be fine, but sometimes     │                  │\n",
       "│ set the solver, like 'lbfgs'. But I think the │ it's good to set the solver, like 'lbfgs'.   │                  │\n",
       "│ default is okay.                              │ But I think the default is okay.             │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Then, fit the model on the training data. So  │ Then, fit the model on the training data. So │                  │\n",
       "│ model.fit(X_train, y_train).                  │ model.fit(X_train, y_train).                 │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ After training, I need to predict the target  │ After training, I need to predict the target │                  │\n",
       "│ for the test set. So y_pred =                 │ for the test set. So y_pred =                │                  │\n",
       "│ model.predict(X_test).                        │ model.predict(X_test).                       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Then, compute the confusion matrix using      │ Then, compute the confusion matrix using     │                  │\n",
       "│ confusion_matrix from sklearn.metrics. So cm  │ confusion_matrix from sklearn.metrics. So cm │                  │\n",
       "│ = confusion_matrix(y_test, y_pred).           │ = confusion_matrix(y_test, y_pred).          │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Now, I need to plot the confusion matrix. How │ Now, I need to plot the confusion matrix.    │                  │\n",
       "│ do I do that? I remember that a heatmap is a  │ How do I do that? I remember that a heatmap  │                  │\n",
       "│ good way to visualize it. So I can use        │ is a good way to visualize it. So I can use  │                  │\n",
       "│ matplotlib's imshow function, or perhaps use  │ matplotlib's imshow function, or perhaps use │                  │\n",
       "│ seaborn's heatmap for better aesthetics.      │ seaborn's heatmap for better aesthetics.     │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Wait, but the function needs to return the    │ Wait, but the function needs to return the   │                  │\n",
       "│ confusion matrix as a numpy array and the     │ confusion matrix as a numpy array and the    │                  │\n",
       "│ Axes object. So I'll need to create a plot,   │ Axes object. So I'll need to create a plot,  │                  │\n",
       "│ make it a figure, plot the confusion matrix,  │ make it a figure, plot the confusion matrix, │                  │\n",
       "│ and then return the axes.                     │ and then return the axes.                    │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ So let's outline the plotting steps:          │ So let's outline the plotting steps:         │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 1. Create a figure and axes. Or just use      │ 1. Create a figure and axes. Or just use     │                  │\n",
       "│ plt.subplots to create a figure and axes      │ plt.subplots to create a figure and axes     │                  │\n",
       "│ object.                                       │ object.                                      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 2. Use sns.heatmap to plot the confusion      │ 2. Use sns.heatmap to plot the confusion     │                  │\n",
       "│ matrix. But wait, the confusion matrix is a   │ matrix. But wait, the confusion matrix is a  │                  │\n",
       "│ numpy array. However, the heatmap function    │ numpy array. However, the heatmap function   │                  │\n",
       "│ can take the data, and I can set parameters   │ can take the data, and I can set parameters  │                  │\n",
       "│ like annot=True to show the numbers, and fmt  │ like annot=True to show the numbers, and fmt │                  │\n",
       "│ to format the numbers as integers.            │ to format the numbers as integers.           │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 3. Set the x-axis labels as 'Predicted' and   │ 3. Set the x-axis labels as 'Predicted' and  │                  │\n",
       "│ the y-axis as 'Actual'.                       │ the y-axis as 'Actual'.                      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 4. Also, set the title of the plot.           │ 4. Also, set the title of the plot.          │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ But wait, how do I get the axes object?       │ But wait, how do I get the axes object?      │                  │\n",
       "│ Because when I create the figure and axes, I  │ Because when I create the figure and axes, I │                  │\n",
       "│ can plot on the axes and then return them.    │ can plot on the axes and then return them.   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Alternatively, I can create a new figure,     │ Alternatively, I can create a new figure,    │                  │\n",
       "│ plot on it, and then return the axes.         │ plot on it, and then return the axes.        │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ So putting it all together:                   │ So putting it all together:                  │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Split the data into training and testing    │ - Split the data into training and testing   │                  │\n",
       "│ sets.                                         │ sets.                                        │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Train the logistic regression model.        │ - Train the logistic regression model.       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Predict on the test set.                    │ - Predict on the test set.                   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Compute the confusion matrix.               │ - Compute the confusion matrix.              │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Create a plot with a heatmap.               │ - Create a plot with a heatmap.              │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Now, let's think about possible issues.       │ Now, let's think about possible issues.      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ What if the target has more than two classes? │ What if the target has more than two         │                  │\n",
       "│ The confusion matrix will handle it, but the  │ classes? The confusion matrix will handle    │                  │\n",
       "│ logistic regression model is for binary       │ it, but the logistic regression model is for │                  │\n",
       "│ classification. So the target should be       │ binary classification. So the target should  │                  │\n",
       "│ binary. So I guess the function assumes that  │ be binary. So I guess the function assumes   │                  │\n",
       "│ the target is binary.                         │ that the target is binary.                   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Another thing: when using train_test_split,   │ Another thing: when using train_test_split,  │                  │\n",
       "│ the test size is                              │ the test size is                             │                  │\n",
       "│ default&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistan… │ default&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assista… │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Ensure all required dependencies are          │ Consider switching to `random.randrange` if  │                  │\n",
       "│ installed, including joblib.&lt;|eot_id|&gt;        │ the correct solution uses it&lt;|eot_id|&gt;       │                  │\n",
       "└───────────────────────────────────────────────┴──────────────────────────────────────────────┴──────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mchosen_text                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrejected_text                               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlogits          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
       "│ <|begin_of_text|><|start_header_id|>system<|… │ <|begin_of_text|><|start_header_id|>system<… │ [0.4907, 0.5093] │\n",
       "│                                               │                                              │                  │\n",
       "│ Cutting Knowledge Date: December 2023         │ Cutting Knowledge Date: December 2023        │                  │\n",
       "│ Today Date: 07 May 2025                       │ Today Date: 07 May 2025                      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ <|eot_id|><|start_header_id|>user<|end_heade… │ <|eot_id|><|start_header_id|>user<|end_head… │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Give me an advice to the problem and answer   │ Give me an advice to the problem and answer  │                  │\n",
       "│ below;                                        │ below;                                       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Problem:Train a logistic regression model on  │ Problem:Train a logistic regression model on │                  │\n",
       "│ one feature and evaluate its performance      │ one feature and evaluate its performance     │                  │\n",
       "│ using a confusion matrix plot. The function   │ using a confusion matrix plot. The function  │                  │\n",
       "│ takes a feature and a target series, splits   │ takes a feature and a target series, splits  │                  │\n",
       "│ them into training and testing sets, trains   │ them into training and testing sets, trains  │                  │\n",
       "│ the logistic regression model, predicts the   │ the logistic regression model, predicts the  │                  │\n",
       "│ target for the test set, and plots the        │ target for the test set, and plots the       │                  │\n",
       "│ confusion matrix.                             │ confusion matrix.                            │                  │\n",
       "│ The function should output with:              │ The function should output with:             │                  │\n",
       "│     (np.ndarray, plt.Axes): A tuple           │     (np.ndarray, plt.Axes): A tuple          │                  │\n",
       "│ containing the confusion matrix and the       │ containing the confusion matrix and the      │                  │\n",
       "│ matplotlib Axes object of the confusion       │ matplotlib Axes object of the confusion      │                  │\n",
       "│ matrix plot.                                  │ matrix plot.                                 │                  │\n",
       "│ You should write self-contained code starting │ You should write self-contained code         │                  │\n",
       "│ with:                                         │ starting with:                               │                  │\n",
       "│ ```                                           │ ```                                          │                  │\n",
       "│ import pandas as pd                           │ import pandas as pd                          │                  │\n",
       "│ from sklearn.model_selection import           │ from sklearn.model_selection import          │                  │\n",
       "│ train_test_split                              │ train_test_split                             │                  │\n",
       "│ from sklearn.linear_model import              │ from sklearn.linear_model import             │                  │\n",
       "│ LogisticRegression                            │ LogisticRegression                           │                  │\n",
       "│ from sklearn.metrics import confusion_matrix  │ from sklearn.metrics import confusion_matrix │                  │\n",
       "│ import numpy as np                            │ import numpy as np                           │                  │\n",
       "│ import matplotlib.pyplot as plt               │ import matplotlib.pyplot as plt              │                  │\n",
       "│ def task_func(feature: pd.Series, target:     │ def task_func(feature: pd.Series, target:    │                  │\n",
       "│ pd.Series) -> (np.ndarray, plt.Axes):         │ pd.Series) -> (np.ndarray, plt.Axes):        │                  │\n",
       "│ ```                                           │ ```                                          │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Answer:<think>                                │ Answer:<think>                               │                  │\n",
       "│ Okay, I need to write a Python function       │ Okay, I need to write a Python function      │                  │\n",
       "│ called task_func that takes a feature and a   │ called task_func that takes a feature and a  │                  │\n",
       "│ target as inputs and returns a tuple          │ target as inputs and returns a tuple         │                  │\n",
       "│ containing a confusion matrix and a           │ containing a confusion matrix and a          │                  │\n",
       "│ matplotlib Axes object. The function should   │ matplotlib Axes object. The function should  │                  │\n",
       "│ train a logistic regression model on one      │ train a logistic regression model on one     │                  │\n",
       "│ feature and evaluate its performance using a  │ feature and evaluate its performance using a │                  │\n",
       "│ confusion matrix plot.                        │ confusion matrix plot.                       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ First, I should think about the steps         │ First, I should think about the steps        │                  │\n",
       "│ involved. Let's break it down.                │ involved. Let's break it down.               │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ The function starts by importing necessary    │ The function starts by importing necessary   │                  │\n",
       "│ libraries, which are already provided. So I   │ libraries, which are already provided. So I  │                  │\n",
       "│ don't need to worry about that.               │ don't need to worry about that.              │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ The first step is to split the feature and    │ The first step is to split the feature and   │                  │\n",
       "│ target into training and testing sets. I      │ target into training and testing sets. I     │                  │\n",
       "│ remember that the train_test_split function   │ remember that the train_test_split function  │                  │\n",
       "│ from sklearn can do this. So I'll use that.   │ from sklearn can do this. So I'll use that.  │                  │\n",
       "│ The feature is a pandas Series, so I need to  │ The feature is a pandas Series, so I need to │                  │\n",
       "│ convert it into a DataFrame or maybe just an  │ convert it into a DataFrame or maybe just an │                  │\n",
       "│ array? Wait, no. Wait, the feature is a       │ array? Wait, no. Wait, the feature is a      │                  │\n",
       "│ single feature, right? So when I split, I can │ single feature, right? So when I split, I    │                  │\n",
       "│ pass the feature as X and target as y.        │ can pass the feature as X and target as y.   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Wait, but train_test_split expects X and y to │ Wait, but train_test_split expects X and y   │                  │\n",
       "│ be arrays. So for the feature, perhaps I      │ to be arrays. So for the feature, perhaps I  │                  │\n",
       "│ should convert the pd.Series into a numpy     │ should convert the pd.Series into a numpy    │                  │\n",
       "│ array. Because when I pass a Series to        │ array. Because when I pass a Series to       │                  │\n",
       "│ train_test_split, it might work, but          │ train_test_split, it might work, but         │                  │\n",
       "│ sometimes it's better to convert it to a      │ sometimes it's better to convert it to a     │                  │\n",
       "│ numpy array. Alternatively, I can reshape it  │ numpy array. Alternatively, I can reshape it │                  │\n",
       "│ into a 2D array since scikit-learn expects 2D │ into a 2D array since scikit-learn expects   │                  │\n",
       "│ for features.                                 │ 2D for features.                             │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Wait, the feature is a single feature, so     │ Wait, the feature is a single feature, so    │                  │\n",
       "│ it's a 1D array. So when I pass it to         │ it's a 1D array. So when I pass it to        │                  │\n",
       "│ train_test_split, I need to make sure it's in │ train_test_split, I need to make sure it's   │                  │\n",
       "│ the correct shape. So perhaps I should        │ in the correct shape. So perhaps I should    │                  │\n",
       "│ reshape it into a 2D array with shape         │ reshape it into a 2D array with shape        │                  │\n",
       "│ (n_samples, 1). Because LogisticRegression    │ (n_samples, 1). Because LogisticRegression   │                  │\n",
       "│ can handle that.                              │ can handle that.                             │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ So, for X, I'll take                          │ So, for X, I'll take                         │                  │\n",
       "│ feature.values.reshape(-1, 1), and y is       │ feature.values.reshape(-1, 1), and y is      │                  │\n",
       "│ target.values.                                │ target.values.                               │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Then, split them into training and testing    │ Then, split them into training and testing   │                  │\n",
       "│ sets. I can set a random state for            │ sets. I can set a random state for           │                  │\n",
       "│ reproducibility, but it's not specified, so   │ reproducibility, but it's not specified, so  │                  │\n",
       "│ maybe I can leave it out or set it to a       │ maybe I can leave it out or set it to a      │                  │\n",
       "│ specific value like 42 for consistency.       │ specific value like 42 for consistency.      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Next, I need to train the logistic regression │ Next, I need to train the logistic           │                  │\n",
       "│ model. So I'll create an instance of          │ regression model. So I'll create an instance │                  │\n",
       "│ LogisticRegression. The default parameters    │ of LogisticRegression. The default           │                  │\n",
       "│ should be fine, but sometimes it's good to    │ parameters should be fine, but sometimes     │                  │\n",
       "│ set the solver, like 'lbfgs'. But I think the │ it's good to set the solver, like 'lbfgs'.   │                  │\n",
       "│ default is okay.                              │ But I think the default is okay.             │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Then, fit the model on the training data. So  │ Then, fit the model on the training data. So │                  │\n",
       "│ model.fit(X_train, y_train).                  │ model.fit(X_train, y_train).                 │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ After training, I need to predict the target  │ After training, I need to predict the target │                  │\n",
       "│ for the test set. So y_pred =                 │ for the test set. So y_pred =                │                  │\n",
       "│ model.predict(X_test).                        │ model.predict(X_test).                       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Then, compute the confusion matrix using      │ Then, compute the confusion matrix using     │                  │\n",
       "│ confusion_matrix from sklearn.metrics. So cm  │ confusion_matrix from sklearn.metrics. So cm │                  │\n",
       "│ = confusion_matrix(y_test, y_pred).           │ = confusion_matrix(y_test, y_pred).          │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Now, I need to plot the confusion matrix. How │ Now, I need to plot the confusion matrix.    │                  │\n",
       "│ do I do that? I remember that a heatmap is a  │ How do I do that? I remember that a heatmap  │                  │\n",
       "│ good way to visualize it. So I can use        │ is a good way to visualize it. So I can use  │                  │\n",
       "│ matplotlib's imshow function, or perhaps use  │ matplotlib's imshow function, or perhaps use │                  │\n",
       "│ seaborn's heatmap for better aesthetics.      │ seaborn's heatmap for better aesthetics.     │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Wait, but the function needs to return the    │ Wait, but the function needs to return the   │                  │\n",
       "│ confusion matrix as a numpy array and the     │ confusion matrix as a numpy array and the    │                  │\n",
       "│ Axes object. So I'll need to create a plot,   │ Axes object. So I'll need to create a plot,  │                  │\n",
       "│ make it a figure, plot the confusion matrix,  │ make it a figure, plot the confusion matrix, │                  │\n",
       "│ and then return the axes.                     │ and then return the axes.                    │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ So let's outline the plotting steps:          │ So let's outline the plotting steps:         │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 1. Create a figure and axes. Or just use      │ 1. Create a figure and axes. Or just use     │                  │\n",
       "│ plt.subplots to create a figure and axes      │ plt.subplots to create a figure and axes     │                  │\n",
       "│ object.                                       │ object.                                      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 2. Use sns.heatmap to plot the confusion      │ 2. Use sns.heatmap to plot the confusion     │                  │\n",
       "│ matrix. But wait, the confusion matrix is a   │ matrix. But wait, the confusion matrix is a  │                  │\n",
       "│ numpy array. However, the heatmap function    │ numpy array. However, the heatmap function   │                  │\n",
       "│ can take the data, and I can set parameters   │ can take the data, and I can set parameters  │                  │\n",
       "│ like annot=True to show the numbers, and fmt  │ like annot=True to show the numbers, and fmt │                  │\n",
       "│ to format the numbers as integers.            │ to format the numbers as integers.           │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 3. Set the x-axis labels as 'Predicted' and   │ 3. Set the x-axis labels as 'Predicted' and  │                  │\n",
       "│ the y-axis as 'Actual'.                       │ the y-axis as 'Actual'.                      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ 4. Also, set the title of the plot.           │ 4. Also, set the title of the plot.          │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ But wait, how do I get the axes object?       │ But wait, how do I get the axes object?      │                  │\n",
       "│ Because when I create the figure and axes, I  │ Because when I create the figure and axes, I │                  │\n",
       "│ can plot on the axes and then return them.    │ can plot on the axes and then return them.   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Alternatively, I can create a new figure,     │ Alternatively, I can create a new figure,    │                  │\n",
       "│ plot on it, and then return the axes.         │ plot on it, and then return the axes.        │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ So putting it all together:                   │ So putting it all together:                  │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Split the data into training and testing    │ - Split the data into training and testing   │                  │\n",
       "│ sets.                                         │ sets.                                        │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Train the logistic regression model.        │ - Train the logistic regression model.       │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Predict on the test set.                    │ - Predict on the test set.                   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Compute the confusion matrix.               │ - Compute the confusion matrix.              │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ - Create a plot with a heatmap.               │ - Create a plot with a heatmap.              │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Now, let's think about possible issues.       │ Now, let's think about possible issues.      │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ What if the target has more than two classes? │ What if the target has more than two         │                  │\n",
       "│ The confusion matrix will handle it, but the  │ classes? The confusion matrix will handle    │                  │\n",
       "│ logistic regression model is for binary       │ it, but the logistic regression model is for │                  │\n",
       "│ classification. So the target should be       │ binary classification. So the target should  │                  │\n",
       "│ binary. So I guess the function assumes that  │ be binary. So I guess the function assumes   │                  │\n",
       "│ the target is binary.                         │ that the target is binary.                   │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Another thing: when using train_test_split,   │ Another thing: when using train_test_split,  │                  │\n",
       "│ the test size is                              │ the test size is                             │                  │\n",
       "│ default<|eot_id|><|start_header_id|>assistan… │ default<|eot_id|><|start_header_id|>assista… │                  │\n",
       "│                                               │                                              │                  │\n",
       "│ Ensure all required dependencies are          │ Consider switching to `random.randrange` if  │                  │\n",
       "│ installed, including joblib.<|eot_id|>        │ the correct solution uses it<|eot_id|>       │                  │\n",
       "└───────────────────────────────────────────────┴──────────────────────────────────────────────┴──────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='2205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  24/2205 06:03 < 10:01:09, 0.06 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.339076</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.230500</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.627400</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.531700</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.303600</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.242800</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.193100</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.064900</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.230300</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.090900</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import RewardConfig\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "batch_size_per_device = 4\n",
    "eval_batch_size_per_device = 4\n",
    "\n",
    "training_args = RewardConfig(\n",
    "    output_dir=model_save_dir,\n",
    "    per_device_train_batch_size=batch_size_per_device,\n",
    "    per_device_eval_batch_size=eval_batch_size_per_device,\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    eval_on_start=True,\n",
    "    save_steps=20,\n",
    "    logging_steps=1,\n",
    "    num_train_epochs = 3,\n",
    "    report_to=None,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    target_modules=[\"k_proj\",\"q_proj\",\"o_proj\", \"v_proj\",\"down_proj\",\"gate_proj\",\"up_proj\",],\n",
    "    layers_to_transform=[25,26,27],\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "rmtrainer.train(formatted_dataset, training_args = training_args, peft_config = peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977846a3-ac1b-4b3f-9452-f76b4db7de08",
   "metadata": {},
   "source": [
    "# Train Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6fbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa08ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
