{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff024e43-f629-4b68-bf88-73b20b235e02",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeab20a-f36e-4915-a06a-6a2a22d88a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following commands in terminal\n",
    "'''\n",
    "cd /workspace\n",
    "pip install \"huggingface_hub[hf_transfer]\"\n",
    "pip install hf_transfer\n",
    "HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download Ray2333/GRM-Llama3.2-3B-rewardmodel-ft --local-dir ./llama3b-rm/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb178240-dc69-40aa-881c-124ef8779b8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers (from -r requirements.txt (line 1))\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting datasets (from -r requirements.txt (line 2))\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl==0.14.0 (from -r requirements.txt (line 3))\n",
      "  Downloading trl-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting peft (from -r requirements.txt (line 4))\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pandas (from -r requirements.txt (line 5))\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib (from -r requirements.txt (line 6))\n",
      "  Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting vllm==0.6.5 (from -r requirements.txt (line 7))\n",
      "  Downloading vllm-0.6.5-cp38-abi3-manylinux1_x86_64.whl.metadata (11 kB)\n",
      "Collecting evaluate (from -r requirements.txt (line 8))\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting sentence_transformers (from -r requirements.txt (line 9))\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate>=0.34.0 (from trl==0.14.0->-r requirements.txt (line 3))\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting rich (from trl==0.14.0->-r requirements.txt (line 3))\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5->-r requirements.txt (line 7)) (5.9.6)\n",
      "Collecting sentencepiece (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5->-r requirements.txt (line 7)) (1.24.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5->-r requirements.txt (line 7)) (2.31.0)\n",
      "Collecting tqdm (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting blake3 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading blake3-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting py-cpuinfo (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting tokenizers>=0.19.1 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting protobuf (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting fastapi!=0.113.*,!=0.114.0,>=0.107.0 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting aiohttp (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting openai>=1.45.0 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading openai-1.76.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn[standard] (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pydantic>=2.9 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5->-r requirements.txt (line 7)) (9.3.0)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5->-r requirements.txt (line 7)) (0.18.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.9 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting outlines==0.1.11 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting xgrammar>=0.1.6 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading xgrammar-0.1.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting typing_extensions>=4.10 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting filelock>=3.16.1 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting partial-json-parser (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5->-r requirements.txt (line 7)) (24.0.1)\n",
      "Collecting msgspec (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf==0.10.0 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (from vllm==0.6.5->-r requirements.txt (line 7)) (4.6.4)\n",
      "Collecting mistral_common>=1.5.0 (from mistral_common[opencv]>=1.5.0->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.5->-r requirements.txt (line 7)) (6.0.1)\n",
      "Collecting einops (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.8.1 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading compressed_tensors-0.8.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting depyf==0.18.0 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting ray>=2.9 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading ray-2.44.1-cp310-cp310-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting nvidia-ml-py>=12.560.30 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting torch==2.5.1 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision==0.20.1 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting xformers==0.0.28.post3 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting astor (from depyf==0.18.0->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dill (from depyf==0.18.0->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting interegular (from outlines==0.1.11->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.6.5->-r requirements.txt (line 7)) (3.1.2)\n",
      "Collecting lark (from outlines==0.1.11->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.6.5->-r requirements.txt (line 7)) (1.5.8)\n",
      "Collecting cloudpickle (from outlines==0.1.11->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting diskcache (from outlines==0.1.11->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.6.5->-r requirements.txt (line 7)) (0.30.2)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.6.5->-r requirements.txt (line 7)) (4.19.2)\n",
      "Collecting pycountry (from outlines==0.1.11->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting airportsdata (from outlines==0.1.11->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading outlines_core-0.1.26-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7)) (3.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7)) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->vllm==0.6.5->-r requirements.txt (line 7)) (1.3.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (23.2)\n",
      "Collecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.3 (from transformers->-r requirements.txt (line 1))\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->-r requirements.txt (line 2))\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill (from depyf==0.18.0->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests>=2.26.0 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets->-r requirements.txt (line 2))\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->-r requirements.txt (line 2))\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 5)) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->-r requirements.txt (line 5))\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->-r requirements.txt (line 5))\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 6))\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 6))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 6))\n",
      "  Downloading fonttools-4.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.5/102.5 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 6))\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.4.7)\n",
      "Collecting scikit-learn (from sentence_transformers->-r requirements.txt (line 9))\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scipy (from sentence_transformers->-r requirements.txt (line 9))\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting starlette<0.47.0,>=0.40.0 (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.5->-r requirements.txt (line 7)) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.30.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fsspec[http]<=2024.12.0,>=2023.1.0 (from datasets->-r requirements.txt (line 2))\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jsonschema (from outlines==0.1.11->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting numpy<2.0.0 (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow (from vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting opencv-python-headless>=4.0.0 (from mistral_common[opencv]>=1.5.0->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.45.0->vllm==0.6.5->-r requirements.txt (line 7)) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.45.0->vllm==0.6.5->-r requirements.txt (line 7)) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>=1.45.0->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.45.0->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.45.0->vllm==0.6.5->-r requirements.txt (line 7)) (1.3.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic>=2.9->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 5)) (1.16.0)\n",
      "Collecting click>=7.0 (from ray>=2.9->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.9->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.5->-r requirements.txt (line 7)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.5->-r requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.5->-r requirements.txt (line 7)) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.5->-r requirements.txt (line 7)) (2022.12.7)\n",
      "Collecting ninja (from xgrammar>=0.1.6->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->trl==0.14.0->-r requirements.txt (line 3))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl==0.14.0->-r requirements.txt (line 3)) (2.16.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers->-r requirements.txt (line 9))\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers->-r requirements.txt (line 9))\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting h11>=0.8 (from uvicorn[standard]->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading watchfiles-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.45.0->vllm==0.6.5->-r requirements.txt (line 7)) (1.1.3)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.45.0->vllm==0.6.5->-r requirements.txt (line 7))\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.6.5->-r requirements.txt (line 7)) (2023.7.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.6.5->-r requirements.txt (line 7)) (0.12.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->trl==0.14.0->-r requirements.txt (line 3))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->outlines==0.1.11->vllm==0.6.5->-r requirements.txt (line 7)) (2.1.2)\n",
      "Downloading trl-0.14.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading vllm-0.6.5-cp38-abi3-manylinux1_x86_64.whl (201.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading compressed_tensors-0.8.1-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading gguf-0.10.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m178.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m244.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m212.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m189.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m144.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m233.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outlines_core-0.1.26-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m131.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m263.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m271.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m166.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m158.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m266.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.7/345.7 kB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fonttools-4.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m223.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m151.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m199.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m224.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m260.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m219.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.76.0-py3-none-any.whl (661 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.2/661.2 kB\u001b[0m \u001b[31m152.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m227.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m192.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.6/443.6 kB\u001b[0m \u001b[31m143.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m253.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m172.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ray-2.44.1-cp310-cp310-manylinux2014_x86_64.whl (67.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.2/316.2 kB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m145.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m197.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m179.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m227.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xgrammar-0.1.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m203.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blake3-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.4/376.4 kB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 kB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m212.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 kB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Downloading jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.9/352.9 kB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.8/219.8 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m168.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m235.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.9/454.9 kB\u001b[0m \u001b[31m154.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.9/333.9 kB\u001b[0m \u001b[31m135.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading airportsdata-20250224-py3-none-any.whl (913 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m211.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m158.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m281.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: sentencepiece, pytz, py-cpuinfo, nvidia-ml-py, blake3, xxhash, websockets, uvloop, tzdata, typing_extensions, tqdm, threadpoolctl, sympy, safetensors, requests, regex, python-dotenv, pycountry, pyarrow, protobuf, propcache, pillow, partial-json-parser, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, msgspec, msgpack, mdurl, lark, kiwisolver, joblib, jiter, interegular, httptools, h11, fsspec, frozenlist, fonttools, filelock, einops, diskcache, dill, cycler, cloudpickle, click, async-timeout, astor, annotated-types, airportsdata, aiohappyeyeballs, watchfiles, uvicorn, typing-inspection, triton, tiktoken, starlette, scipy, pydantic-core, pandas, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, multidict, markdown-it-py, huggingface-hub, httpcore, gguf, depyf, contourpy, aiosignal, yarl, tokenizers, scikit-learn, rich, pydantic, prometheus-fastapi-instrumentator, nvidia-cusolver-cu12, matplotlib, jsonschema, httpx, transformers, torch, ray, outlines_core, openai, mistral_common, lm-format-enforcer, fastapi, aiohttp, xgrammar, xformers, torchvision, sentence_transformers, outlines, compressed-tensors, accelerate, vllm, peft, datasets, trl, evaluate\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.3.0\n",
      "    Uninstalling Pillow-9.3.0:\n",
      "      Successfully uninstalled Pillow-9.3.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.9.0\n",
      "    Uninstalling filelock-3.9.0:\n",
      "      Successfully uninstalled filelock-3.9.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.19.2\n",
      "    Uninstalling jsonschema-4.19.2:\n",
      "      Successfully uninstalled jsonschema-4.19.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.16.0+cu118\n",
      "    Uninstalling torchvision-0.16.0+cu118:\n",
      "      Successfully uninstalled torchvision-0.16.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.6.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 airportsdata-20250224 annotated-types-0.7.0 astor-0.8.1 async-timeout-5.0.1 blake3-1.0.4 click-8.1.8 cloudpickle-3.1.1 compressed-tensors-0.8.1 contourpy-1.3.2 cycler-0.12.1 datasets-3.5.0 depyf-0.18.0 dill-0.3.8 diskcache-5.6.3 einops-0.8.1 evaluate-0.4.3 fastapi-0.115.12 filelock-3.18.0 fonttools-4.57.0 frozenlist-1.6.0 fsspec-2024.12.0 gguf-0.10.0 h11-0.16.0 httpcore-1.0.9 httptools-0.6.4 httpx-0.28.1 huggingface-hub-0.30.2 interegular-0.3.3 jiter-0.9.0 joblib-1.4.2 jsonschema-4.23.0 kiwisolver-1.4.8 lark-1.2.2 lm-format-enforcer-0.10.11 markdown-it-py-3.0.0 matplotlib-3.10.1 mdurl-0.1.2 mistral_common-1.5.4 msgpack-1.1.0 msgspec-0.19.0 multidict-6.4.3 multiprocess-0.70.16 ninja-1.11.1.4 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-ml-py-12.570.86 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 openai-1.76.0 opencv-python-headless-4.11.0.86 outlines-0.1.11 outlines_core-0.1.26 pandas-2.2.3 partial-json-parser-0.2.1.1.post5 peft-0.15.2 pillow-11.2.1 prometheus-fastapi-instrumentator-7.1.0 propcache-0.3.1 protobuf-6.30.2 py-cpuinfo-9.0.0 pyarrow-20.0.0 pycountry-24.6.1 pydantic-2.11.3 pydantic-core-2.33.1 python-dotenv-1.1.0 pytz-2025.2 ray-2.44.1 regex-2024.11.6 requests-2.32.3 rich-14.0.0 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 sentence_transformers-4.1.0 sentencepiece-0.2.0 starlette-0.46.2 sympy-1.13.1 threadpoolctl-3.6.0 tiktoken-0.9.0 tokenizers-0.21.1 torch-2.5.1 torchvision-0.20.1 tqdm-4.67.1 transformers-4.51.3 triton-3.1.0 trl-0.14.0 typing-inspection-0.4.0 typing_extensions-4.13.2 tzdata-2025.2 uvicorn-0.34.2 uvloop-0.21.0 vllm-0.6.5 watchfiles-1.0.5 websockets-15.0.1 xformers-0.0.28.post3 xgrammar-0.1.18 xxhash-3.5.0 yarl-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807277fd-70a0-48ae-aedc-d0929691741f",
   "metadata": {},
   "source": [
    "# Search Keys from Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1837433-c24b-4147-968e-ab6229f60d10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-28 01:07:58 config.py:2171] Casting torch.float16 to torch.bfloat16.\n",
      "INFO 04-28 01:07:58 config.py:2167] Downcasting torch.float32 to torch.bfloat16.\n",
      "WARNING 04-28 01:08:05 arg_utils.py:1096] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "WARNING 04-28 01:08:05 config.py:596] Async output processing can not be enabled with pipeline parallel\n",
      "INFO 04-28 01:08:05 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='RM-converted-model', speculative_config=None, tokenizer='RM-converted-model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=RM-converted-model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=<rmsearch.rmsearch.pooler_config object at 0x7f377e3624a0>, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 04-28 01:08:06 multiproc_worker_utils.py:312] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 04-28 01:08:06 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 04-28 01:08:07 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4310)\u001b[0;0m INFO 04-28 01:08:07 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4310)\u001b[0;0m INFO 04-28 01:08:07 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "INFO 04-28 01:08:08 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 04-28 01:08:08 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4310)\u001b[0;0m INFO 04-28 01:08:08 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4310)\u001b[0;0m INFO 04-28 01:08:08 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 04-28 01:08:08 model_runner.py:1092] Starting to load model RM-converted-model...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4310)\u001b[0;0m INFO 04-28 01:08:08 model_runner.py:1092] Starting to load model RM-converted-model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a680cf9466f740ea89ca74178a406dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=4310)\u001b[0;0m INFO 04-28 01:08:21 model_runner.py:1097] Loading model weights took 3.3908 GB\n",
      "INFO 04-28 01:08:24 model_runner.py:1097] Loading model weights took 3.3908 GB\n"
     ]
    }
   ],
   "source": [
    "from rmsearch import Search\n",
    "search = Search(model_name = \"/workspace/llama3b-rm\",\n",
    "        tensor_parallel_size = 1,\n",
    "        pipeline_parallel_size = 2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe3c6c1-4fa2-47a7-a9d1-331b493a1a62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function Search.get_relevance.<locals>.format at 0x7f38b5766f80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa330404b954f9fb4313f87fcc23e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 0.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 1.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 2.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 3.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 4.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 5.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 6.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 7.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 8.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 9.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 10.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 11.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 12.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 13.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 14.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 15.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 16.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 17.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 18.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 19.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 20.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 21.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 22.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 23.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 24.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 25.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 26.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 27.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 28.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 29.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 30.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 31.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 32.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 33.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 34.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 35.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 36.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 37.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 38.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 39.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 40.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 41.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 42.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 43.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 44.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 45.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 46.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 47.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 48.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 49.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 50.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 51.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 52.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 53.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 54.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 55.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 56.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 57.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 58.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 59.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 60.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 61.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 62.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 63.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 64.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 65.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 66.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 67.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 68.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 69.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 70.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 71.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 72.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 73.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 74.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 75.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 76.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 77.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 78.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 79.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 80.\n",
      "INFO 04-28 01:08:24 async_llm_engine.py:211] Added request 81.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 82.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 83.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 84.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 85.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 86.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 87.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 88.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 89.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 90.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 91.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 92.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 93.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 94.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 95.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 96.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 97.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 98.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:211] Added request 99.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W428 01:08:25.077712294 ProcessGroupNCCL.cpp:3057] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[rank1]:[W428 01:08:25.077885846 ProcessGroupNCCL.cpp:3057] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 0.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 2.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 4.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 6.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 8.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 10.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 12.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 14.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 16.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 18.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 20.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 22.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 24.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 26.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 28.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 30.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 32.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 34.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 36.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 38.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 40.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 42.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 44.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 46.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 48.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 50.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 52.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 54.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 56.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 58.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 60.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 62.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 64.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 66.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 68.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 70.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 72.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 74.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 76.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 78.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 80.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 82.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 84.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 86.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 88.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 90.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 92.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 94.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 96.\n",
      "INFO 04-28 01:08:25 async_llm_engine.py:179] Finished request 98.\n",
      "PoolingRequestOutput(request_id=0, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=2, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=4, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=6, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=8, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=10, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=12, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=14, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=16, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=18, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=20, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=22, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=24, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=26, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=28, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=30, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=32, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=34, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=36, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=38, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=40, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=42, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=44, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=46, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=48, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=50, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=52, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=54, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=56, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=58, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=60, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=62, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=64, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=66, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=68, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=70, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=72, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=74, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=76, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=78, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=80, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=82, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=84, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=86, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=88, outputs=PoolingOutput(data=tensor([ 0.4180, -0.1641,  0.2148,  ..., -1.4531, -2.2344, -0.3301],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=90, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=92, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=94, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=96, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=98, outputs=PoolingOutput(data=tensor([ 0.3906, -0.1699,  0.1641,  ..., -0.9805, -1.9844, -0.2402],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 4178, 44, 374, 20902, 11688, 5008, 902, 649, 387, 1903, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 1.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 3.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 5.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 7.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 9.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 11.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 13.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 15.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 17.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 19.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 21.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 23.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 25.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 27.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 29.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 31.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 33.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 35.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 37.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 39.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 41.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 43.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 45.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 47.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 49.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 51.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 53.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 55.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 57.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 59.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 61.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 63.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 65.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 67.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 69.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 71.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 73.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 75.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 77.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 79.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 81.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 83.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 85.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 87.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 89.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 91.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 93.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 95.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 97.\n",
      "INFO 04-28 01:08:26 async_llm_engine.py:179] Finished request 99.\n",
      "PoolingRequestOutput(request_id=1, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=3, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=5, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=7, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=9, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=11, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=13, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=15, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=17, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=19, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=21, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=23, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=25, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=27, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=29, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=31, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=33, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=35, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=37, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=39, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=41, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=43, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=45, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=47, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=49, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=51, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=53, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=55, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=57, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=59, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=61, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=63, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=65, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=67, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=69, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=71, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=73, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=75, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=77, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=79, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=81, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=83, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=85, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=87, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=89, outputs=PoolingOutput(data=tensor([ 0.0530, -0.1543,  0.1816,  ..., -1.2031, -2.0156, -0.3809],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 4438, 311, 1304, 445, 11237, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=91, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=93, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=95, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=97, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "PoolingRequestOutput(request_id=99, outputs=PoolingOutput(data=tensor([ 0.2275,  0.3770, -0.2930,  ..., -1.1016, -2.0469, -0.3262],\n",
      "       device='cuda:1', dtype=torch.bfloat16)), prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 5186, 220, 2366, 20, 271, 128009, 128006, 882, 128007, 271, 36227, 757, 264, 1401, 430, 9248, 279, 3319, 3770, 401, 2929, 25, 3923, 596, 279, 6864, 315, 6457, 30, 128009, 128006, 78191, 128007, 271, 52566, 6864, 374, 2564, 128009], finished=True)\n",
      "torch.Size([1, 3072])\n",
      "torch.Size([3072, 1])\n",
      "[{'query': 'How to make LLM?', 'query_id': 0, 'keys': [{'key_id': 8, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 6, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 0, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 4, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 2, 'key': 'LLM is Large Language Model which can be made ...'}]}, {'query': \"What's the capital of Japan?\", 'query_id': 1, 'keys': [{'key_id': 5, 'key': 'Japanese capital is ...'}, {'key_id': 9, 'key': 'Japanese capital is ...'}, {'key_id': 7, 'key': 'Japanese capital is ...'}, {'key_id': 1, 'key': 'Japanese capital is ...'}, {'key_id': 3, 'key': 'Japanese capital is ...'}]}, {'query': 'How to make LLM?', 'query_id': 2, 'keys': [{'key_id': 8, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 6, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 0, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 4, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 2, 'key': 'LLM is Large Language Model which can be made ...'}]}, {'query': \"What's the capital of Japan?\", 'query_id': 3, 'keys': [{'key_id': 5, 'key': 'Japanese capital is ...'}, {'key_id': 9, 'key': 'Japanese capital is ...'}, {'key_id': 7, 'key': 'Japanese capital is ...'}, {'key_id': 1, 'key': 'Japanese capital is ...'}, {'key_id': 3, 'key': 'Japanese capital is ...'}]}, {'query': 'How to make LLM?', 'query_id': 4, 'keys': [{'key_id': 8, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 6, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 0, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 4, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 2, 'key': 'LLM is Large Language Model which can be made ...'}]}, {'query': \"What's the capital of Japan?\", 'query_id': 5, 'keys': [{'key_id': 5, 'key': 'Japanese capital is ...'}, {'key_id': 9, 'key': 'Japanese capital is ...'}, {'key_id': 7, 'key': 'Japanese capital is ...'}, {'key_id': 1, 'key': 'Japanese capital is ...'}, {'key_id': 3, 'key': 'Japanese capital is ...'}]}, {'query': 'How to make LLM?', 'query_id': 6, 'keys': [{'key_id': 8, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 6, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 0, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 4, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 2, 'key': 'LLM is Large Language Model which can be made ...'}]}, {'query': \"What's the capital of Japan?\", 'query_id': 7, 'keys': [{'key_id': 5, 'key': 'Japanese capital is ...'}, {'key_id': 9, 'key': 'Japanese capital is ...'}, {'key_id': 7, 'key': 'Japanese capital is ...'}, {'key_id': 1, 'key': 'Japanese capital is ...'}, {'key_id': 3, 'key': 'Japanese capital is ...'}]}, {'query': 'How to make LLM?', 'query_id': 8, 'keys': [{'key_id': 8, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 6, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 0, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 4, 'key': 'LLM is Large Language Model which can be made ...'}, {'key_id': 2, 'key': 'LLM is Large Language Model which can be made ...'}]}, {'query': \"What's the capital of Japan?\", 'query_id': 9, 'keys': [{'key_id': 5, 'key': 'Japanese capital is ...'}, {'key_id': 9, 'key': 'Japanese capital is ...'}, {'key_id': 7, 'key': 'Japanese capital is ...'}, {'key_id': 1, 'key': 'Japanese capital is ...'}, {'key_id': 3, 'key': 'Japanese capital is ...'}]}]\n"
     ]
    }
   ],
   "source": [
    "queries = [\"How to make LLM?\", \"What's the capital of Japan?\"] * 5\n",
    "keys = [\"LLM is Large Language Model which can be made ...\", \"Japanese capital is ...\"] * 5\n",
    "output = await search(queries, keys)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "458bd83c-804f-4bbf-9308-1899bd6adad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'How to make LLM?',\n",
       "  'query_id': 0,\n",
       "  'keys': [{'key_id': 8,\n",
       "    'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 6, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 0, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 4, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 2, 'key': 'LLM is Large Language Model which can be made ...'}]},\n",
       " {'query': \"What's the capital of Japan?\",\n",
       "  'query_id': 1,\n",
       "  'keys': [{'key_id': 5, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 9, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 7, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 1, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 3, 'key': 'Japanese capital is ...'}]},\n",
       " {'query': 'How to make LLM?',\n",
       "  'query_id': 2,\n",
       "  'keys': [{'key_id': 8,\n",
       "    'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 6, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 0, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 4, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 2, 'key': 'LLM is Large Language Model which can be made ...'}]},\n",
       " {'query': \"What's the capital of Japan?\",\n",
       "  'query_id': 3,\n",
       "  'keys': [{'key_id': 5, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 9, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 7, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 1, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 3, 'key': 'Japanese capital is ...'}]},\n",
       " {'query': 'How to make LLM?',\n",
       "  'query_id': 4,\n",
       "  'keys': [{'key_id': 8,\n",
       "    'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 6, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 0, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 4, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 2, 'key': 'LLM is Large Language Model which can be made ...'}]},\n",
       " {'query': \"What's the capital of Japan?\",\n",
       "  'query_id': 5,\n",
       "  'keys': [{'key_id': 5, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 9, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 7, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 1, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 3, 'key': 'Japanese capital is ...'}]},\n",
       " {'query': 'How to make LLM?',\n",
       "  'query_id': 6,\n",
       "  'keys': [{'key_id': 8,\n",
       "    'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 6, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 0, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 4, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 2, 'key': 'LLM is Large Language Model which can be made ...'}]},\n",
       " {'query': \"What's the capital of Japan?\",\n",
       "  'query_id': 7,\n",
       "  'keys': [{'key_id': 5, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 9, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 7, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 1, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 3, 'key': 'Japanese capital is ...'}]},\n",
       " {'query': 'How to make LLM?',\n",
       "  'query_id': 8,\n",
       "  'keys': [{'key_id': 8,\n",
       "    'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 6, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 0, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 4, 'key': 'LLM is Large Language Model which can be made ...'},\n",
       "   {'key_id': 2, 'key': 'LLM is Large Language Model which can be made ...'}]},\n",
       " {'query': \"What's the capital of Japan?\",\n",
       "  'query_id': 9,\n",
       "  'keys': [{'key_id': 5, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 9, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 7, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 1, 'key': 'Japanese capital is ...'},\n",
       "   {'key_id': 3, 'key': 'Japanese capital is ...'}]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42637ff3-54d0-419e-9a51-a26a4da7b2e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808b853d-bbae-4c4c-93c8-52e7705e864a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-27 12:51:05 config.py:2167] Downcasting torch.float32 to torch.float16.\n",
      "INFO 04-27 12:51:12 config.py:478] This model supports multiple tasks: {'score', 'generate', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 04-27 12:51:12 arg_utils.py:1086] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 04-27 12:51:12 config.py:1364] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 04-27 12:51:12 config.py:596] Async output processing can not be enabled with pipeline parallel\n",
      "INFO 04-27 12:51:12 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='RM-converted-model', speculative_config=None, tokenizer='RM-converted-model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=RM-converted-model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=False, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 04-27 12:51:13 multiproc_worker_utils.py:312] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 04-27 12:51:13 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 04-27 12:51:14 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1325)\u001b[0;0m INFO 04-27 12:51:14 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1325)\u001b[0;0m INFO 04-27 12:51:14 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "INFO 04-27 12:51:15 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 04-27 12:51:15 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1325)\u001b[0;0m INFO 04-27 12:51:15 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1325)\u001b[0;0m INFO 04-27 12:51:15 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 04-27 12:51:15 model_runner.py:1092] Starting to load model RM-converted-model...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1325)\u001b[0;0m INFO 04-27 12:51:15 model_runner.py:1092] Starting to load model RM-converted-model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af17223461fa4122b9365a15e59fbd35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1325)\u001b[0;0m INFO 04-27 12:51:30 model_runner.py:1097] Loading model weights took 3.3908 GB\n",
      "INFO 04-27 12:51:36 model_runner.py:1097] Loading model weights took 3.3908 GB\n",
      "INFO 04-27 12:51:37 worker.py:241] Memory profiling takes 0.48 seconds\n",
      "INFO 04-27 12:51:37 worker.py:241] the current vLLM instance can use total_gpu_memory (44.34GiB) x gpu_memory_utilization (0.95) = 42.12GiB\n",
      "INFO 04-27 12:51:37 worker.py:241] model weights take 3.39GiB; non_torch_memory takes 0.21GiB; PyTorch activation peak memory takes 0.14GiB; the rest of the memory reserved for KV Cache is 38.38GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1325)\u001b[0;0m INFO 04-27 12:51:37 worker.py:241] Memory profiling takes 0.55 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1325)\u001b[0;0m INFO 04-27 12:51:37 worker.py:241] the current vLLM instance can use total_gpu_memory (44.34GiB) x gpu_memory_utilization (0.95) = 42.12GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1325)\u001b[0;0m INFO 04-27 12:51:37 worker.py:241] model weights take 3.39GiB; non_torch_memory takes 0.21GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 37.31GiB.\n",
      "INFO 04-27 12:51:37 distributed_gpu_executor.py:57] # GPU blocks: 43667, # CPU blocks: 4681\n",
      "INFO 04-27 12:51:37 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 5.33x\n",
      "INFO 04-27 12:51:40 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-27 12:51:40 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1325)\u001b[0;0m INFO 04-27 12:51:41 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1325)\u001b[0;0m INFO 04-27 12:51:41 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1325)\u001b[0;0m INFO 04-27 12:52:08 model_runner.py:1527] Graph capturing finished in 28 secs, took 0.16 GiB\n",
      "INFO 04-27 12:52:09 model_runner.py:1527] Graph capturing finished in 29 secs, took 0.16 GiB\n",
      "INFO 04-27 12:52:09 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 32.59 seconds\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 0.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 1.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 2.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 3.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 4.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 5.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 6.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 7.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 8.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 9.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 10.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 11.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 12.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 13.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 14.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 15.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 16.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 17.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 18.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 19.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 20.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 21.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 22.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 23.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 24.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 25.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 26.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 27.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 28.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 29.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 30.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 31.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 32.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 33.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 34.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 35.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 36.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 37.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 38.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 39.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 40.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 41.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 42.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 43.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 44.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 45.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 46.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 47.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 48.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 49.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 50.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 51.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 52.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 53.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 54.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 55.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 56.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 57.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 58.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 59.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 60.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 61.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 62.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 63.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 64.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 65.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 66.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 67.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 68.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 69.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 70.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 71.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 72.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 73.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 74.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 75.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 76.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 77.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 78.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 79.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 80.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 81.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 82.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 83.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 84.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 85.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 86.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 87.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 88.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 89.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 90.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 91.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 92.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 93.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 94.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 95.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 96.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 97.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 98.\n",
      "INFO 04-27 12:52:10 async_llm_engine.py:211] Added request 99.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/vllm/distributed/parallel_state.py:530: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:1560.)\n",
      "  object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)\n",
      "[rank0]:[W427 12:52:10.482942781 ProcessGroupNCCL.cpp:3057] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[rank1]:[W427 12:52:10.483297152 ProcessGroupNCCL.cpp:3057] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-27 12:52:15 metrics.py:467] Avg prompt throughput: 119.5 tokens/s, Avg generation throughput: 1275.0 tokens/s, Running: 100 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 04-27 12:52:20 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1612.2 tokens/s, Running: 100 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 04-27 12:52:25 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1238.7 tokens/s, Running: 100 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 04-27 12:52:30 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 811.3 tokens/s, Running: 100 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 04-27 12:52:35 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 678.6 tokens/s, Running: 100 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 04-27 12:52:40 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 659.0 tokens/s, Running: 100 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 04-27 12:52:45 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1062.1 tokens/s, Running: 100 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 04-27 12:52:50 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1198.7 tokens/s, Running: 100 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 04-27 12:52:55 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1179.1 tokens/s, Running: 100 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 0.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 2.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 4.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 6.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 8.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 10.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 12.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 14.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 16.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 18.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 20.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 22.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 24.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 26.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 28.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 30.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 32.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 34.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 36.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 38.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 40.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 42.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 44.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 46.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 48.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 50.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 52.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 54.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 56.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 58.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 60.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 62.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 64.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 66.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 68.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 70.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 72.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 74.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 76.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 78.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 80.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 82.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 84.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 86.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 88.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 90.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 92.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 94.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 96.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 98.\n",
      "INFO 04-27 12:53:00 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1111.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 1.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 3.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 5.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 7.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 9.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 11.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 13.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 15.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 17.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 19.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 21.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 23.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 25.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 27.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 29.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 31.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 33.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 35.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 37.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 39.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 41.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 43.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 45.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 47.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 49.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 51.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 53.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 55.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 57.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 59.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 61.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 63.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 65.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 67.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 69.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 71.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 73.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 75.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 77.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 79.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 81.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 83.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 85.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 87.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 89.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 91.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 93.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 95.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 97.\n",
      "INFO 04-27 12:53:00 async_llm_engine.py:179] Finished request 99.\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams\n",
    "\n",
    "model_name = \"RM-converted-model\"\n",
    "tensor_parallel_size = 1\n",
    "pipeline_parallel_size = 2\n",
    "\n",
    "class pooler_config:\n",
    "    def __init__(self):\n",
    "        self.pooling_type = \"LAST\"\n",
    "        self.normalize = False\n",
    "        self.softmax = False\n",
    "        self.softmax = False\n",
    "        self.step_tag_id = None\n",
    "        self.returned_token_ids = None\n",
    "\n",
    "pooler_config_ = pooler_config()\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model = model_name,\n",
    "    dtype=\"bfloat16\",\n",
    "    tensor_parallel_size = tensor_parallel_size,\n",
    "    pipeline_parallel_size = pipeline_parallel_size,\n",
    "    distributed_executor_backend = \"mp\",\n",
    "    gpu_memory_utilization=0.95,\n",
    "    task=\"embed\",\n",
    "    override_pooler_config=pooler_config_,\n",
    ")\n",
    "engine = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "prompts = [\"What is LLM?\"]*100\n",
    "\n",
    "async def process(prompt, i):\n",
    "    # start the generation\n",
    "    final_output = None\n",
    "    results_generator = engine.generate(prompt, SamplingParams(temperature=0, max_tokens=3000), i)\n",
    "    async for request_output in results_generator:\n",
    "        # print(request_output) => for streaming\n",
    "        final_output = request_output\n",
    "\n",
    "    output = final_output.outputs[0].text\n",
    "    \n",
    "    return output\n",
    "\n",
    "outputs = await asyncio.gather(\n",
    "    *[process(prompt, i) for i, prompt in enumerate(prompts)]\n",
    ")\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfbf236e-a912-4f7a-a509-37b1bd28c766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (Large Language Model)\n",
      "A Large Language Model (LLM) is a type of artificial intelligence (AI) model that is designed to process and understand human language. LLMs are trained on vast amounts of text data and can generate human-like text, answer questions, and even engage in conversations. They are a key component of many modern AI applications, including chatbots, virtual assistants, and language translation software.\n",
      "LLMs are typically trained using a technique called masked language modeling, which involves randomly masking a portion of the input text and then predicting the missing words or phrases. This process allows the model to learn the patterns and structures of language, as well as the relationships between words and concepts.\n",
      "There are many different types of LLMs, including:\n",
      "1. **Transformers**: These are the most common type of LLM and are based on the transformer architecture, which was introduced in 2017. Transformers are designed to process sequential data, such as text, and are particularly well-suited for tasks such as language translation and text summarization.\n",
      "2. **BERT**: BERT (Bidirectional Encoder Representations from Transformers) is a type of LLM that is designed to process text in a more human-like way. BERT is trained on a large corpus of text data and can generate text that is more coherent and natural-sounding than other LLMs.\n",
      "3. **RoBERTa**: RoBERTa is a type of LLM that is designed to process text in a more efficient way. RoBERTa is trained on a large corpus of text data and can generate text that is more coherent and natural-sounding than other LLMs.\n",
      "4. **XLNet**: XLNet is a type of LLM that is designed to process text in a more efficient way. XLNet is trained on a large corpus of text data and can generate text that is more coherent and natural-sounding than other LLMs.\n",
      "5. **T5**: T5 is a type of LLM that is designed to process text in a more efficient way. T5 is trained on a large corpus of text data and can generate text that is more coherent and natural-sounding than other LLMs.\n",
      "I hope this helps! Let me know if you have any other questions.\n"
     ]
    }
   ],
   "source": [
    "output = final_output.outputs[0].text\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0bafec-f2b1-4fc4-bbe1-6fc37806ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vLLM import AsyncLLMEngine, engine_args\n",
    "engine = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "example_input = {\n",
    "    \"input\": \"What is LLM?\",\n",
    "    \"request_id\": 0,\n",
    "}\n",
    "# start the generation\n",
    "results_generator = engine.encode(\n",
    "   example_input[\"input\"],\n",
    "   PoolingParams(),\n",
    "   example_input[\"request_id\"])\n",
    "# get the results\n",
    "final_output = None\n",
    "async for request_output in results_generator:\n",
    "    if await request.is_disconnected():\n",
    "        # Abort the request if the client disconnects.\n",
    "        await engine.abort(request_id)\n",
    "\n",
    "    final_output = request_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e95bc-3a07-4526-8702-47fb63656362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
